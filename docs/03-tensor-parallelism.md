å¼ é‡å¹¶è¡Œ (Tensor Parallelism)
ğŸ“‹ ç›®å½•

* æ ¸å¿ƒåŸç†

* Megatron-LMæ–¹æ³•

* åˆ—å¹¶è¡Œä¸è¡Œå¹¶è¡Œ

* Transformerå¹¶è¡ŒåŒ–

* é€šä¿¡ä¼˜åŒ–

* å®ç°ç»†èŠ‚

* æ€§èƒ½åˆ†æ

* å®æˆ˜æ¡ˆä¾‹

æ ¸å¿ƒåŸç†
ä»€ä¹ˆæ˜¯å¼ é‡å¹¶è¡Œï¼Ÿ

```
åŸºæœ¬æ€æƒ³:
å°†å•ä¸ªå±‚çš„æƒé‡çŸ©é˜µåˆ†å‰²åˆ°å¤šä¸ªGPUä¸Š
æ¯ä¸ªGPUè®¡ç®—éƒ¨åˆ†ç»“æœ
é€šè¿‡é›†åˆé€šä¿¡åˆå¹¶æœ€ç»ˆè¾“å‡º
```

ä¸ºä»€ä¹ˆéœ€è¦å¼ é‡å¹¶è¡Œï¼Ÿ
é—®é¢˜: å•ä¸ªå±‚å¤ªå¤§ï¼Œä¸€ä¸ªGPUè£…ä¸ä¸‹

```python
# ä¾‹å¦‚: è¶…å¤§çº¿æ€§å±‚
layer = nn.Linear(12288, 49152)  # 12B Ã— 49K

å‚æ•°é‡: 12,288 Ã— 49,152 = 603M å‚æ•°
å†…å­˜: 603M Ã— 4 bytes (FP32) = 2.4 GB (ä»…æƒé‡!)

åŠ ä¸Šæ¿€æ´»å€¼ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€:
æ€»å†…å­˜ â‰ˆ 2.4 GB Ã— 16 = 38.4 GB

å•ä¸ªA100 80GB: å¯ä»¥è£…ä¸‹
ä½†100B+æ¨¡å‹çš„å±‚: è£…ä¸ä¸‹ï¼
```

å¼ é‡å¹¶è¡Œ vs æ•°æ®å¹¶è¡Œ

```
æ•°æ®å¹¶è¡Œ:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPU 0       â”‚  â”‚ GPU 1       â”‚  â”‚ GPU 2       â”‚
â”‚ å®Œæ•´æ¨¡å‹    â”‚  â”‚ å®Œæ•´æ¨¡å‹    â”‚  â”‚ å®Œæ•´æ¨¡å‹    â”‚
â”‚ Data[0-31]  â”‚  â”‚ Data[32-63] â”‚  â”‚ Data[64-95] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å¼ é‡å¹¶è¡Œ:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPU 0       â”‚  â”‚ GPU 1       â”‚  â”‚ GPU 2       â”‚
â”‚ æƒé‡[0:1/3] â”‚  â”‚ æƒé‡[1/3:2/3]â”‚ â”‚ æƒé‡[2/3:1] â”‚
â”‚ å®Œæ•´æ•°æ®    â”‚  â”‚ å®Œæ•´æ•°æ®    â”‚  â”‚ å®Œæ•´æ•°æ®    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Megatron-LMæ–¹æ³•
æ ¸å¿ƒæ€æƒ³
Megatron-LM: NVIDIAæå‡ºçš„Transformerå¼ é‡å¹¶è¡Œæ–¹æ³•

```
å…³é”®æ´å¯Ÿ:
1. Transformerä¸­çš„çŸ©é˜µä¹˜æ³•å¯ä»¥æŒ‰åˆ—æˆ–æŒ‰è¡Œåˆ‡åˆ†
2. å¤šå¤´æ³¨æ„åŠ›å¤©ç„¶é€‚åˆå¹¶è¡Œ (æ¯ä¸ªheadç‹¬ç«‹)
3. ç²¾å¿ƒè®¾è®¡åˆ‡åˆ†æ–¹å¼ï¼Œæœ€å°åŒ–é€šä¿¡
```

ä¸¤ç§åŸºæœ¬åˆ‡åˆ†æ–¹å¼
1. åˆ—å¹¶è¡Œ (Column Parallel)

```python
"""
åˆ—å¹¶è¡Œ: æŒ‰è¾“å‡ºç»´åº¦åˆ‡åˆ†

Y = XW, å…¶ä¸­ W: [k, n]
åˆ‡åˆ† W = [Wâ‚€, Wâ‚, Wâ‚‚, Wâ‚ƒ]

æ¯ä¸ªGPUè®¡ç®—éƒ¨åˆ†è¾“å‡º:
GPU 0: Yâ‚€ = X @ Wâ‚€  â†’ [batch, n/4]
GPU 1: Yâ‚ = X @ Wâ‚  â†’ [batch, n/4]
GPU 2: Yâ‚‚ = X @ Wâ‚‚  â†’ [batch, n/4]
GPU 3: Yâ‚ƒ = X @ Wâ‚ƒ  â†’ [batch, n/4]

æœ€ç»ˆè¾“å‡º: Y = [Yâ‚€, Yâ‚, Yâ‚‚, Yâ‚ƒ]  (æ‹¼æ¥)
é€šä¿¡: æ— éœ€é€šä¿¡! (ä»…éœ€æ‹¼æ¥)
"""
```

å¯è§†åŒ–:

```
è¾“å…¥ X [batchÃ—k]:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  â”‚
â”‚    X (å®Œæ•´)      â”‚
â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æƒé‡ W [kÃ—n]:
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
â”‚ Wâ‚€ â”‚ Wâ‚ â”‚ Wâ‚‚ â”‚ Wâ‚ƒ â”‚  â† æŒ‰åˆ—åˆ‡åˆ†
â”‚    â”‚    â”‚    â”‚    â”‚
â”‚    â”‚    â”‚    â”‚    â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜
GPU0  GPU1 GPU2  GPU3

è¾“å‡º Y [batchÃ—n]:
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
â”‚ Yâ‚€ â”‚ Yâ‚ â”‚ Yâ‚‚ â”‚ Yâ‚ƒ â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜
```

2. è¡Œå¹¶è¡Œ (Row Parallel)

```python
"""
è¡Œå¹¶è¡Œ: æŒ‰è¾“å…¥ç»´åº¦åˆ‡åˆ†

Y = XW, å…¶ä¸­ W: [k, n]
åˆ‡åˆ† W æŒ‰è¡Œ:
  â”Œ Wâ‚€ â”
W=â”‚ Wâ‚ â”‚
  â”‚ Wâ‚‚ â”‚
  â”” Wâ‚ƒ â”˜

éœ€è¦å…ˆåˆ‡åˆ†è¾“å…¥ X:
GPU 0: Yâ‚€ = Xâ‚€ @ Wâ‚€  â†’ [batch, n]
GPU 1: Yâ‚ = Xâ‚ @ Wâ‚  â†’ [batch, n]
GPU 2: Yâ‚‚ = Xâ‚‚ @ Wâ‚‚  â†’ [batch, n]
GPU 3: Yâ‚ƒ = Xâ‚ƒ @ Wâ‚ƒ  â†’ [batch, n]

æœ€ç»ˆè¾“å‡º: Y = Yâ‚€ + Yâ‚ + Yâ‚‚ + Yâ‚ƒ  (AllReduceæ±‚å’Œ)
é€šä¿¡: éœ€è¦AllReduce!
"""
```

å¯è§†åŒ–:

```
è¾“å…¥ X [batchÃ—k] (éœ€å…ˆåˆ‡åˆ†):
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
â”‚ Xâ‚€ â”‚ Xâ‚ â”‚ Xâ‚‚ â”‚ Xâ‚ƒ â”‚  â† æŒ‰ç‰¹å¾ç»´åˆ‡åˆ†
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜
GPU0  GPU1 GPU2  GPU3

æƒé‡ W [kÃ—n]:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Wâ‚€         â”‚  GPU 0
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       Wâ‚         â”‚  GPU 1
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       Wâ‚‚         â”‚  GPU 2
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       Wâ‚ƒ         â”‚  GPU 3
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

éƒ¨åˆ†ç»“æœ:
GPU 0: Yâ‚€ [batchÃ—n]
GPU 1: Yâ‚ [batchÃ—n]
GPU 2: Yâ‚‚ [batchÃ—n]
GPU 3: Yâ‚ƒ [batchÃ—n]
        â†“
    AllReduce
        â†“
Y = Yâ‚€+Yâ‚+Yâ‚‚+Yâ‚ƒ [batchÃ—n]
```

åˆ—å¹¶è¡Œä¸è¡Œå¹¶è¡Œ
ä»£ç å®ç°

```python
"""
åˆ—å¹¶è¡Œå’Œè¡Œå¹¶è¡Œçš„PyTorchå®ç°
"""

import torch
import torch.nn as nn
import torch.distributed as dist


class ColumnParallelLinear(nn.Module):
    """åˆ—å¹¶è¡Œçº¿æ€§å±‚"""
    
    def __init__(
        self, 
        input_size: int,
        output_size: int,
        tensor_parallel_group,
        bias: bool = True
    ):
        super().__init__()
        
        self.input_size = input_size
        self.output_size = output_size
        self.tensor_parallel_group = tensor_parallel_group
        
        # è·å–å¹¶è¡Œç»„ä¿¡æ¯
        world_size = dist.get_world_size(tensor_parallel_group)
        rank = dist.get_rank(tensor_parallel_group)
        
        # è®¡ç®—æ¯ä¸ªGPUçš„è¾“å‡ºç»´åº¦
        assert output_size % world_size == 0
        self.output_size_per_partition = output_size // world_size
        
        # åˆ›å»ºæƒé‡ (åªå­˜å‚¨è‡ªå·±çš„åˆ†ç‰‡)
        self.weight = nn.Parameter(
            torch.empty(
                self.output_size_per_partition,
                self.input_size
            )
        )
        
        if bias:
            self.bias = nn.Parameter(
                torch.empty(self.output_size_per_partition)
            )
        else:
            self.register_parameter('bias', None)
        
        # åˆå§‹åŒ–
        self._initialize_weights()
    
    def forward(self, input_: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input_: [batch, seq_len, input_size]
        
        Returns:
            output: [batch, seq_len, output_size_per_partition]
        """
        # åˆ—å¹¶è¡Œ: Y = X @ W^T
        # æ¯ä¸ªGPUè®¡ç®—éƒ¨åˆ†è¾“å‡º
        output = torch.matmul(input_, self.weight.t())
        
        if self.bias is not None:
            output = output + self.bias
        
        return output
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        nn.init.xavier_uniform_(self.weight)
        if self.bias is not None:
            nn.init.zeros_(self.bias)


class RowParallelLinear(nn.Module):
    """è¡Œå¹¶è¡Œçº¿æ€§å±‚"""
    
    def __init__(
        self,
        input_size: int,
        output_size: int,
        tensor_parallel_group,
        bias: bool = True,
        input_is_parallel: bool = False
    ):
        super().__init__()
        
        self.input_size = input_size
        self.output_size = output_size
        self.tensor_parallel_group = tensor_parallel_group
        self.input_is_parallel = input_is_parallel
        
        # è·å–å¹¶è¡Œç»„ä¿¡æ¯
        world_size = dist.get_world_size(tensor_parallel_group)
        rank = dist.get_rank(tensor_parallel_group)
        
        # è®¡ç®—æ¯ä¸ªGPUçš„è¾“å…¥ç»´åº¦
        assert input_size % world_size == 0
        self.input_size_per_partition = input_size // world_size
        
        # åˆ›å»ºæƒé‡ (åªå­˜å‚¨è‡ªå·±çš„åˆ†ç‰‡)
        self.weight = nn.Parameter(
            torch.empty(
                output_size,
                self.input_size_per_partition
            )
        )
        
        # Biasåªåœ¨rank 0åˆ›å»º (é¿å…é‡å¤)
        if bias and rank == 0:
            self.bias = nn.Parameter(torch.empty(output_size))
        else:
            self.register_parameter('bias', None)
        
        self._initialize_weights()
    
    def forward(self, input_: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input_: [batch, seq_len, input_size] æˆ–
                   [batch, seq_len, input_size_per_partition] (å¦‚æœå·²åˆ‡åˆ†)
        
        Returns:
            output: [batch, seq_len, output_size]
        """
        # å¦‚æœè¾“å…¥è¿˜æ²¡åˆ‡åˆ†ï¼Œå…ˆåˆ‡åˆ†
        if not self.input_is_parallel:
            input_ = self._split_along_last_dim(input_)
        
        # è¡Œå¹¶è¡Œ: Y_i = X_i @ W_i^T
        # æ¯ä¸ªGPUè®¡ç®—éƒ¨åˆ†ç»“æœ
        output_parallel = torch.matmul(input_, self.weight.t())
        
        # AllReduceæ±‚å’Œ
        output = self._reduce_from_tensor_parallel_region(
            output_parallel
        )
        
        # æ·»åŠ bias (åªåœ¨rank 0)
        if self.bias is not None:
            output = output + self.bias
        
        return output
    
    def _split_along_last_dim(self, tensor: torch.Tensor) -> torch.Tensor:
        """æŒ‰æœ€åä¸€ç»´åˆ‡åˆ†tensor"""
        world_size = dist.get_world_size(self.tensor_parallel_group)
        rank = dist.get_rank(self.tensor_parallel_group)
        
        last_dim = tensor.size(-1)
        assert last_dim % world_size == 0
        
        chunk_size = last_dim // world_size
        start_idx = rank * chunk_size
        end_idx = start_idx + chunk_size
        
        return tensor[..., start_idx:end_idx].contiguous()
    
    def _reduce_from_tensor_parallel_region(
        self, 
        input_: torch.Tensor
    ) -> torch.Tensor:
        """ä»å¼ é‡å¹¶è¡ŒåŒºåŸŸreduce"""
        # AllReduceæ±‚å’Œ
        dist.all_reduce(
            input_,
            op=dist.ReduceOp.SUM,
            group=self.tensor_parallel_group
        )
        return input_
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        nn.init.xavier_uniform_(self.weight)
        if self.bias is not None:
            nn.init.zeros_(self.bias)
```

é€šä¿¡åˆ†æ

```python
"""
åˆ—å¹¶è¡Œ vs è¡Œå¹¶è¡Œ é€šä¿¡å¯¹æ¯”
"""

# åˆ—å¹¶è¡Œ
"""
å‰å‘: æ— é€šä¿¡ (ä»…æ‹¼æ¥)
åå‘: AllReduceæ¢¯åº¦ (âˆ‡X)

é€šä¿¡é‡: input_size Ã— hidden_size Ã— sizeof(dtype)
"""

# è¡Œå¹¶è¡Œ
"""
å‰å‘: AllReduceè¾“å‡º (Y)
åå‘: æ— é€šä¿¡ (æ¢¯åº¦å·²åˆ‡åˆ†)

é€šä¿¡é‡: batch_size Ã— seq_len Ã— output_size Ã— sizeof(dtype)
"""

# ç»“è®º: 
# - åˆ—å¹¶è¡Œ: åå‘é€šä¿¡
# - è¡Œå¹¶è¡Œ: å‰å‘é€šä¿¡
# - å·§å¦™ç»„åˆå¯ä»¥æœ€å°åŒ–é€šä¿¡!
```

Transformerå¹¶è¡ŒåŒ–
MLPå±‚å¹¶è¡ŒåŒ–

```python
"""
Transformer MLPå±‚çš„å¼ é‡å¹¶è¡Œ

æ ‡å‡†MLP:
    h1 = GeLU(X @ W1 + b1)  # [batch, seq, 4*hidden]
    h2 = h1 @ W2 + b2        # [batch, seq, hidden]

å¹¶è¡ŒåŒ–ç­–ç•¥:
    W1: åˆ—å¹¶è¡Œ (è¾“å‡º4*hiddenåˆ‡åˆ†)
    W2: è¡Œå¹¶è¡Œ (è¾“å…¥4*hiddenåˆ‡åˆ†)
"""


class ParallelMLP(nn.Module):
    """å¹¶è¡ŒMLP"""
    
    def __init__(self, hidden_size, ffn_hidden_size, tp_group):
        super().__init__()
        
        # W1: åˆ—å¹¶è¡Œ
        # hidden_size â†’ ffn_hidden_size (åˆ‡åˆ†è¾“å‡º)
        self.dense_h_to_4h = ColumnParallelLinear(
            hidden_size,
            ffn_hidden_size,
            tensor_parallel_group=tp_group,
            bias=True
        )
        
        self.activation = nn.GELU()
        
        # W2: è¡Œå¹¶è¡Œ
        # ffn_hidden_size â†’ hidden_size (åˆ‡åˆ†è¾“å…¥)
        self.dense_4h_to_h = RowParallelLinear(
            ffn_hidden_size,
            hidden_size,
            tensor_parallel_group=tp_group,
            bias=True,
            input_is_parallel=True  # è¾“å…¥å·²åˆ‡åˆ†
        )
    
    def forward(self, hidden_states):
        """
        å‰å‘ä¼ æ’­
        
        é€šä¿¡åˆ†æ:
        1. dense_h_to_4h (åˆ—å¹¶è¡Œ): 
           - å‰å‘: æ— é€šä¿¡
           - åå‘: AllReduce âˆ‡hidden_states
        
        2. activation: ç‹¬ç«‹è®¡ç®—ï¼Œæ— é€šä¿¡
        
        3. dense_4h_to_h (è¡Œå¹¶è¡Œ):
           - å‰å‘: AllReduce output
           - åå‘: æ— é€šä¿¡
        
        æ€»é€šä¿¡: 2æ¬¡AllReduce (å‰å‘1æ¬¡ + åå‘1æ¬¡)
        """
        # [batch, seq, hidden] â†’ [batch, seq, ffn_hidden/tp_size]
        intermediate = self.dense_h_to_4h(hidden_states)
        
        # æ¿€æ´»å‡½æ•° (æ— é€šä¿¡)
        intermediate = self.activation(intermediate)
        
        # [batch, seq, ffn_hidden/tp_size] â†’ [batch, seq, hidden]
        # å†…éƒ¨AllReduce
        output = self.dense_4h_to_h(intermediate)
        
        return output
```

æ³¨æ„åŠ›å±‚å¹¶è¡ŒåŒ–

```python
"""
Multi-Head Attentionçš„å¼ é‡å¹¶è¡Œ

æ ‡å‡†Attention:
    Q = X @ W_Q  # [batch, seq, num_heads * head_dim]
    K = X @ W_K
    V = X @ W_V
    
    # åˆ†å¤´
    Q = Q.view(batch, seq, num_heads, head_dim)
    K = K.view(batch, seq, num_heads, head_dim)
    V = V.view(batch, seq, num_heads, head_dim)
    
    # è®¡ç®—attention
    scores = Q @ K.transpose(-2, -1) / sqrt(head_dim)
    attn = softmax(scores)
    output = attn @ V
    
    # åˆå¹¶å¤´
    output = output.view(batch, seq, num_heads * head_dim)
    output = output @ W_O

å¹¶è¡ŒåŒ–ç­–ç•¥:
    W_Q, W_K, W_V: åˆ—å¹¶è¡Œ (æŒ‰headåˆ‡åˆ†)
    W_O: è¡Œå¹¶è¡Œ (è¾“å…¥å·²æŒ‰headåˆ‡åˆ†)
"""


class ParallelAttention(nn.Module):
    """å¹¶è¡Œæ³¨æ„åŠ›å±‚"""
    
    def __init__(
        self, 
        hidden_size, 
        num_attention_heads,
        tp_group
    ):
        super().__init__()
        
        self.hidden_size = hidden_size
        self.num_attention_heads = num_attention_heads
        self.tp_group = tp_group
        
        # å¼ é‡å¹¶è¡Œç»„ä¿¡æ¯
        tp_world_size = dist.get_world_size(tp_group)
        
        # ç¡®ä¿headæ•°å¯ä»¥æ•´é™¤
        assert num_attention_heads % tp_world_size == 0
        self.num_attention_heads_per_partition = (
            num_attention_heads // tp_world_size
        )
        
        self.head_dim = hidden_size // num_attention_heads
        self.hidden_size_per_partition = (
            self.num_attention_heads_per_partition * self.head_dim
        )
        
        # QKVæŠ•å½±: åˆ—å¹¶è¡Œ
        # è¾“å‡ºç»´åº¦ = 3 * hidden_size (Q, K, Væ‹¼æ¥)
        # åˆ‡åˆ†åæ¯ä¸ªGPU: 3 * hidden_size_per_partition
        self.query_key_value = ColumnParallelLinear(
            hidden_size,
            3 * hidden_size,
            tensor_parallel_group=tp_group,
            bias=True
        )
        
        # è¾“å‡ºæŠ•å½±: è¡Œå¹¶è¡Œ
        self.dense = RowParallelLinear(
            hidden_size,
            hidden_size,
            tensor_parallel_group=tp_group,
            bias=True,
            input_is_parallel=True
        )
    
    def forward(self, hidden_states, attention_mask=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            hidden_states: [batch, seq_len, hidden_size]
            attention_mask: [batch, 1, seq_len, seq_len]
        
        Returns:
            output: [batch, seq_len, hidden_size]
        """
        batch_size, seq_len, _ = hidden_states.size()
        
        # QKVæŠ•å½± (åˆ—å¹¶è¡Œ)
        # [batch, seq, hidden] â†’ [batch, seq, 3*hidden_per_partition]
        qkv = self.query_key_value(hidden_states)
        
        # åˆ‡åˆ†æˆQ, K, V
        qkv = qkv.view(
            batch_size,
            seq_len,
            self.num_attention_heads_per_partition,
            3 * self.head_dim
        )
        
        # [batch, seq, num_heads_per_partition, 3*head_dim]
        # â†’ 3 Ã— [batch, num_heads_per_partition, seq, head_dim]
        q, k, v = torch.chunk(qkv, 3, dim=-1)
        q = q.permute(0, 2, 1, 3)
        k = k.permute(0, 2, 1, 3)
        v = v.permute(0, 2, 1, 3)
        
        # è®¡ç®—attention scores
        # [batch, num_heads_per_partition, seq, seq]
        scores = torch.matmul(q, k.transpose(-2, -1))
        scores = scores / (self.head_dim ** 0.5)
        
        # åº”ç”¨mask
        if attention_mask is not None:
            scores = scores + attention_mask
        
        # Softmax
        attn_weights = torch.softmax(scores, dim=-1)
        
        # åº”ç”¨attention
        # [batch, num_heads_per_partition, seq, head_dim]
        context = torch.matmul(attn_weights, v)
        
        # è½¬ç½®å›æ¥å¹¶åˆå¹¶heads
        # [batch, seq, num_heads_per_partition, head_dim]
        context = context.permute(0, 2, 1, 3).contiguous()
        
        # [batch, seq, hidden_per_partition]
        context = context.view(
            batch_size,
            seq_len,
            self.hidden_size_per_partition
        )
        
        # è¾“å‡ºæŠ•å½± (è¡Œå¹¶è¡Œ)
        # [batch, seq, hidden_per_partition] â†’ [batch, seq, hidden]
        output = self.dense(context)
        
        return output
```

å®Œæ•´Transformer Block

```python
"""
å®Œæ•´çš„å¼ é‡å¹¶è¡ŒTransformer Block
"""


class ParallelTransformerBlock(nn.Module):
    """å¹¶è¡ŒTransformerå—"""
    
    def __init__(
        self,
        hidden_size,
        num_attention_heads,
        ffn_hidden_size,
        tp_group
    ):
        super().__init__()
        
        # LayerNorm (æ— éœ€å¹¶è¡Œ)
        self.input_layernorm = nn.LayerNorm(hidden_size)
        
        # æ³¨æ„åŠ›å±‚ (å¼ é‡å¹¶è¡Œ)
        self.attention = ParallelAttention(
            hidden_size,
            num_attention_heads,
            tp_group
        )
        
        # LayerNorm
        self.post_attention_layernorm = nn.LayerNorm(hidden_size)
        
        # MLPå±‚ (å¼ é‡å¹¶è¡Œ)
        self.mlp = ParallelMLP(
            hidden_size,
            ffn_hidden_size,
            tp_group
        )
    
    def forward(self, hidden_states, attention_mask=None):
        """
        å‰å‘ä¼ æ’­
        
        é€šä¿¡åˆ†æ (æ¯ä¸ªblock):
        1. Attention:
           - QKVæŠ•å½± (åˆ—å¹¶è¡Œ): åå‘AllReduce
           - è¾“å‡ºæŠ•å½± (è¡Œå¹¶è¡Œ): å‰å‘AllReduce
        
        2. MLP:
           - ç¬¬ä¸€å±‚ (åˆ—å¹¶è¡Œ): åå‘AllReduce
           - ç¬¬äºŒå±‚ (è¡Œå¹¶è¡Œ): å‰å‘AllReduce
        
        æ€»è®¡: 4æ¬¡AllReduce (å‰å‘2æ¬¡ + åå‘2æ¬¡)
        """
        # æ³¨æ„åŠ› + æ®‹å·®
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        attention_output = self.attention(hidden_states, attention_mask)
        hidden_states = residual + attention_output
        
        # MLP + æ®‹å·®
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        mlp_output = self.mlp(hidden_states)
        hidden_states = residual + mlp_output
        
        return hidden_states
```

é€šä¿¡ä¼˜åŒ–
1. é€šä¿¡ä¸è®¡ç®—é‡å 

```python
"""
ä½¿ç”¨å¼‚æ­¥é€šä¿¡é‡å è®¡ç®—
"""

class OptimizedRowParallelLinear(nn.Module):
    """ä¼˜åŒ–çš„è¡Œå¹¶è¡Œçº¿æ€§å±‚"""
    
    def forward(self, input_: torch.Tensor) -> torch.Tensor:
        # å±€éƒ¨è®¡ç®—
        output_parallel = torch.matmul(input_, self.weight.t())
        
        # å¼‚æ­¥AllReduce
        handle = dist.all_reduce(
            output_parallel,
            op=dist.ReduceOp.SUM,
            group=self.tensor_parallel_group,
            async_op=True  # å¼‚æ­¥
        )
        
        # å¯ä»¥åœ¨è¿™é‡Œåšå…¶ä»–è®¡ç®—...
        # ä¾‹å¦‚: dropout, biasç­‰
        
        # ç­‰å¾…é€šä¿¡å®Œæˆ
        handle.wait()
        
        if self.bias is not None:
            output_parallel = output_parallel + self.bias
        
        return output_parallel
```

2. é€šä¿¡èåˆ

```python
"""
å°†å¤šä¸ªå°é€šä¿¡åˆå¹¶æˆä¸€ä¸ªå¤§é€šä¿¡
"""

def fused_all_reduce(tensors, group):
    """èåˆå¤šä¸ªtensorçš„AllReduce"""
    
    # å°†æ‰€æœ‰tensoræ‹¼æ¥æˆä¸€ä¸ªå¤§tensor
    sizes = [t.numel() for t in tensors]
    flat_tensors = [t.flatten() for t in tensors]
    fused_tensor = torch.cat(flat_tensors)
    
    # ä¸€æ¬¡AllReduce
    dist.all_reduce(fused_tensor, op=dist.ReduceOp.SUM, group=group)
    
    # åˆ‡åˆ†å›åŸæ¥çš„tensor
    offset = 0
    for i, size in enumerate(sizes):
        tensors[i].copy_(
            fused_tensor[offset:offset + size].view_as(tensors[i])
        )
        offset += size
```

3. å‡å°‘é€šä¿¡é¢‘ç‡

```python
"""
æ¢¯åº¦ç´¯ç§¯ + å‡å°‘AllReduceé¢‘ç‡
"""

class CommunicationOptimizedModel(nn.Module):
    def __init__(self, model, accumulation_steps, tp_group):
        super().__init__()
        self.model = model
        self.accumulation_steps = accumulation_steps
        self.tp_group = tp_group
        self.tp_world_size = dist.get_world_size(tp_group)
        self.step_count = 0
    
    def forward(self, *args, **kwargs):
        output = self.model(*args, **kwargs)
        
        # æ¯accumulation_stepsæ‰æ‰§è¡Œä¸€æ¬¡AllReduce
        self.step_count += 1
        if self.step_count % self.accumulation_steps == 0:
            self._all_reduce_gradients()
        
        return output
    
    def _all_reduce_gradients(self):
        """AllReduceæ‰€æœ‰æ¢¯åº¦"""
        for param in self.model.parameters():
            if param.grad is not None:
                dist.all_reduce(
                    param.grad,
                    op=dist.ReduceOp.SUM,
                    group=self.tp_group
                )
                param.grad.div_(self.tp_world_size)
```

å®ç°ç»†èŠ‚
1. åˆå§‹åŒ–åŒæ­¥

```python
"""
ç¡®ä¿æ‰€æœ‰GPUçš„æƒé‡åˆå§‹åŒ–ç›¸åŒ
"""

def synchronized_init(tensor, group):
    """åŒæ­¥åˆå§‹åŒ–"""
    rank = dist.get_rank(group)
    
    if rank == 0:
        # åªåœ¨rank 0åˆå§‹åŒ–
        nn.init.xavier_uniform_(tensor)
    
    # å¹¿æ’­åˆ°æ‰€æœ‰è¿›ç¨‹
    dist.broadcast(tensor, src=0, group=group)
```

2. éšæœºæ•°ç§å­ç®¡ç†

```python
"""
å¼ é‡å¹¶è¡Œéœ€è¦ç›¸åŒçš„dropout maskç­‰éšæœºæ“ä½œ
"""

def set_parallel_seed(base_seed, tp_rank):
    """ä¸ºå¼ é‡å¹¶è¡Œè®¾ç½®ç›¸åŒç§å­"""
    torch.manual_seed(base_seed + tp_rank * 1000)  # ç®€å•æ–¹å¼ç¡®ä¿ç›¸åŒ

class ParallelDropout(nn.Dropout):
    def forward(self, input_):
        if not self.training:
            return input_
        
        # æ‰€æœ‰rankä½¿ç”¨ç›¸åŒç§å­
        seed = torch.initial_seed()  # æˆ–ä½¿ç”¨å…¨å±€step
        torch.manual_seed(seed)
        return super().forward(input_)
```

3. æ£€æŸ¥ç‚¹ä¿å­˜/åŠ è½½

```python
"""
å¼ é‡å¹¶è¡Œæ¨¡å‹çš„æ£€æŸ¥ç‚¹å¤„ç†
"""

def save_tensor_parallel_checkpoint(model, path, tp_rank, tp_size):
    """æ¯ä¸ªrankä¿å­˜è‡ªå·±çš„åˆ†ç‰‡"""
    state_dict = model.state_dict()
    checkpoint = {
        'model_state_dict': state_dict,
        'tp_rank': tp_rank,
        'tp_size': tp_size,
    }
    torch.save(checkpoint, f"{path}/rank_{tp_rank:02d}.pt")

def load_tensor_parallel_checkpoint(model, path, tp_rank):
    """åŠ è½½å¯¹åº”rankçš„åˆ†ç‰‡"""
    ckpt_path = f"{path}/rank_{tp_rank:02d}.pt"
    checkpoint = torch.load(ckpt_path, map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])
    return model
```

æ€§èƒ½åˆ†æ
é€šä¿¡å¼€é”€åˆ†æ

```python
"""
ç†è®ºé€šä¿¡æ—¶é—´ä¼°ç®—
"""

def estimate_communication_time(
    hidden_size: int,
    seq_len: int,
    batch_size: int,
    tp_size: int,
    num_layers: int = 1,
    bandwidth_gbps: float = 600  # NVLinkå…¸å‹å¸¦å®½ ~600 GB/s (åŒå‘)
):
    """
    ä¼°ç®—ä¸€ä¸ªTransformer Blockçš„é€šä¿¡æ—¶é—´ï¼ˆå•ä½ï¼šç§’ï¼‰
    
    å‡è®¾FP16ç²¾åº¦ï¼Œ2 bytes/element
    ä½¿ç”¨Ring AllReduceç®—æ³•ï¼ˆNCCLé»˜è®¤ï¼‰
    """
    bytes_per_element = 2
    
    # æ¯ä¸ªBlockçº¦4æ¬¡AllReduceï¼ˆAttention + MLPå„2æ¬¡ï¼‰
    num_allreduce = 4 * num_layers
    
    # æ¯æ¬¡AllReduceä¼ è¾“çš„æ•°æ®é‡ï¼ˆæ¿€æ´»ï¼‰
    data_per_allreduce = batch_size * seq_len * hidden_size * bytes_per_element  # bytes
    
    # Ring AllReduceé€šä¿¡é‡ = 2 * (tp_size - 1) / tp_size * data
    ring_factor = 2 * (tp_size - 1) / tp_size
    total_bytes = num_allreduce * data_per_allreduce * ring_factor
    
    # å¸¦å®½è½¬æ¢ä¸ºbytes/s
    bandwidth_bytes_per_sec = bandwidth_gbps * 1e9 / 8
    
    comm_time_sec = total_bytes / bandwidth_bytes_per_sec
    
    return comm_time_sec

# ç¤ºä¾‹ï¼šGPT-3 175Bè§„æ¨¡ï¼Œtp=8, A100 NVLink
print(estimate_communication_time(
    hidden_size=12288,
    seq_len=2048,
    batch_size=1,
    tp_size=8,
    num_layers=96
))  # â‰ˆ 0.15-0.3ç§’ï¼ˆå–å†³äºå…·ä½“é…ç½®ï¼‰
```

å®é™…æ€§èƒ½å¯¹æ¯”ï¼ˆå…¸å‹A100 8å¡ï¼ŒFP16ï¼‰

| é…ç½®               | æ¨¡å‹è§„æ¨¡ | TP Size | MFU (Model FLOPs Utilization) | é€šä¿¡å æ¯” |
|--------------------|----------|---------|-------------------------------|----------|
| æ•°æ®å¹¶è¡Œ           | 13B     | 1       | ~55%                          | ä½       |
| å¼ é‡å¹¶è¡Œ (Megatron)| 175B    | 8       | ~48-52%                       | 15-25%   |
| 3Då¹¶è¡Œ (TP+PP+DP)  | 175B    | 8       | ~58%                          | <10%     |

å…³é”®ç»“è®ºï¼š
- TP=8æ—¶ï¼Œé€šä¿¡å¼€é”€é€šå¸¸å æ€»æ—¶é—´çš„15-30%
- é€šè¿‡å¼‚æ­¥é€šä¿¡ + é€šä¿¡èåˆï¼Œå¯å°†é€šä¿¡å æ¯”é™è‡³<10%
- åºåˆ—è¶Šé•¿ã€batchè¶Šå°ï¼Œé€šä¿¡å æ¯”è¶Šé«˜ï¼ˆæ¿€æ´»é€šä¿¡ä¸»å¯¼ï¼‰

å®æˆ˜æ¡ˆä¾‹
1. Megatron-LM (NVIDIAå®˜æ–¹)

- æœ€æ—©å®ç°å¼ é‡å¹¶è¡Œçš„æ¡†æ¶
- æ”¯æŒGPT-3 175Båœ¨1024 A100ä¸Šè®­ç»ƒ
- ä»£ç ï¼šhttps://github.com/NVIDIA/Megatron-LM
- å…³é”®ç‰¹æ€§ï¼šå®Œæ•´çš„åˆ—/è¡Œå¹¶è¡Œå®ç°ã€é€šä¿¡é‡å ã€æ¨¡å‹æ£€æŸ¥ç‚¹åˆ†ç‰‡

2. DeepSpeed (Microsoft)

- é›†æˆå¼ é‡å¹¶è¡Œ + ZeRO + Pipelineå¹¶è¡Œ
- æ”¯æŒLlama-70Båœ¨å•èŠ‚ç‚¹8å¡é«˜æ•ˆè®­ç»ƒ
- æ›´æ˜“ç”¨APIï¼Œè‡ªåŠ¨å¤„ç†é€šä¿¡ä¼˜åŒ–

3. HuggingFace + Accelerate + Megatroné›†æˆ

- ç¤¾åŒºä¸»æµæ–¹æ¡ˆï¼štransformers + megatron-lmæ’ä»¶
- ç¤ºä¾‹è®­ç»ƒ70Bæ¨¡å‹ï¼š
```bash
deepspeed --num_gpus=8 train.py \
    --deepspeed ds_config_zero3.json \
    --tensor-parallel-size 8
```

4. å®é™…éƒ¨ç½²ç»éªŒæ€»ç»“

- TPå¤§å°æ¨èï¼š8ä¸ºæœ€ä½³ç”œç‚¹ï¼ˆNVLinkå…¨äº’è”ï¼‰
- è¶…è¿‡8å¡å»ºè®®ç»“åˆPipelineå¹¶è¡Œï¼ˆ3Då¹¶è¡Œï¼‰
- é•¿åºåˆ—ï¼ˆ>4096ï¼‰æ—¶æ³¨æ„æ¿€æ´»å†…å­˜çˆ†ç‚¸ï¼Œå¯ç»“åˆåºåˆ—å¹¶è¡Œ
- æ¨ç†é˜¶æ®µï¼šå¼ é‡å¹¶è¡Œå¯æ˜¾è‘—é™ä½å•å¡å†…å­˜éœ€æ±‚ï¼ˆå¦‚70Bæ¨¡å‹åªéœ€4Ã—A100ï¼‰

å¼ é‡å¹¶è¡Œæ˜¯è®­ç»ƒè¶…å¤§è§„æ¨¡Transformeræ¨¡å‹çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œé€šè¿‡å·§å¦™çš„çŸ©é˜µåˆ‡åˆ†å’Œé€šä¿¡ä¼˜åŒ–ï¼Œå®ç°äº†æ¨¡å‹å‚æ•°åœ¨å¤šGPUé—´çš„æœ‰æ•ˆåˆ†å¸ƒï¼Œæ˜¯ä»13Båˆ°ä¸‡äº¿å‚æ•°æ¨¡å‹æ¼”è¿›çš„å…³é”®ä½¿èƒ½æŠ€æœ¯ã€‚