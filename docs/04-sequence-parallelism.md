### åºåˆ—å¹¶è¡Œ (Sequence Parallelism)

ğŸ“‹ ç›®å½•

* æ ¸å¿ƒåŸç†

* ä¸å¼ é‡å¹¶è¡Œçš„åŒºåˆ«

* å®ç°æ–¹å¼

* Transformerä¸­çš„åº”ç”¨

* é€šä¿¡ä¼˜åŒ–

* æ€§èƒ½åˆ†æ

* å®æˆ˜æ¡ˆä¾‹

#### æ ¸å¿ƒåŸç†
ä»€ä¹ˆæ˜¯åºåˆ—å¹¶è¡Œï¼Ÿ

```
åŸºæœ¬æ€æƒ³:
å°†è¾“å…¥åºåˆ—ç»´åº¦ï¼ˆsequence dimensionï¼‰åˆ‡åˆ†åˆ°å¤šä¸ªGPUä¸Š
æ¯ä¸ªGPUåªå¤„ç†åºåˆ—çš„ä¸€éƒ¨åˆ†
é€šè¿‡AllGather/ReduceScatterç­‰é€šä¿¡åˆå¹¶æ¿€æ´»å€¼
ä¸»è¦ç›®æ ‡: å‡å°‘æ¿€æ´»å†…å­˜å ç”¨ï¼Œæ”¯æŒè¶…é•¿åºåˆ—è®­ç»ƒ
```

ä¸ºä»€ä¹ˆéœ€è¦åºåˆ—å¹¶è¡Œï¼Ÿ
é—®é¢˜: é•¿ä¸Šä¸‹æ–‡æ¨¡å‹æ¿€æ´»å†…å­˜çˆ†ç‚¸

```python
# ä¾‹å¦‚: é•¿åºåˆ—æ³¨æ„åŠ›
seq_len = 128000  # 128K tokens
hidden_size = 8192
batch_size = 1

# KV cache: 2 Ã— seq_len Ã— hidden_size Ã— heads Ã— head_dim Ã— bytes
# FP16: â‰ˆ seq_len Ã— hidden_size Ã— 2 bytes Ã— 2 (K+V)
å†…å­˜ â‰ˆ 128K Ã— 8192 Ã— 4 bytes â‰ˆ 4 GB (ä»…å•å±‚KV!)

å¤šå±‚ + æ¿€æ´»: è½»æ¾è¶…å•å¡80GBé™åˆ¶
```

åºåˆ—å¹¶è¡Œ vs å¼ é‡å¹¶è¡Œ

```
å¼ é‡å¹¶è¡Œ (TP):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPU 0       â”‚  â”‚ GPU 1       â”‚
â”‚ æƒé‡åˆ‡åˆ†    â”‚  â”‚ æƒé‡åˆ‡åˆ†    â”‚
â”‚ å®Œæ•´åºåˆ—    â”‚  â”‚ å®Œæ•´åºåˆ—    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

åºåˆ—å¹¶è¡Œ (SP):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPU 0       â”‚  â”‚ GPU 1       â”‚
â”‚ å®Œæ•´æƒé‡    â”‚  â”‚ å®Œæ•´æƒé‡    â”‚
â”‚ åºåˆ—[0:L/2] â”‚  â”‚ åºåˆ—[L/2:L] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ä¸å¼ é‡å¹¶è¡Œçš„åŒºåˆ«
- **å¼ é‡å¹¶è¡Œ**ï¼šåˆ‡åˆ†æƒé‡çŸ©é˜µï¼Œä¸»è¦è§£å†³æ¨¡å‹å‚æ•°å¤ªå¤§é—®é¢˜
- **åºåˆ—å¹¶è¡Œ**ï¼šåˆ‡åˆ†æ¿€æ´»ï¼ˆactivationsï¼‰ï¼Œä¸»è¦è§£å†³é•¿åºåˆ—æ¿€æ´»å†…å­˜é—®é¢˜
- **é€šå¸¸ç»„åˆä½¿ç”¨**ï¼šSPå¿…é¡»ä¸TPç»“åˆï¼ˆMegatroné£æ ¼ï¼‰ï¼Œå› ä¸ºSPä¾èµ–TPçš„é€šä¿¡æ¨¡å¼ä¼˜åŒ–AllReduce

å¯è§†åŒ–å¯¹æ¯”ï¼ˆAttentionå±‚ï¼‰:

```
TP alone:
å®Œæ•´åºåˆ— â†’ æ‰€æœ‰GPUè®¡ç®—å®Œæ•´æ¿€æ´» â†’ AllReduceåˆå¹¶

SP + TP:
åˆ‡åˆ†åºåˆ— â†’ æ¯ä¸ªGPUè®¡ç®—éƒ¨åˆ†æ¿€æ´» â†’ ReduceScatter/AllGather
â†’ æ¿€æ´»å†…å­˜å‡å°‘ ~TP_sizeå€
```

#### å®ç°æ–¹å¼
å¸¸è§ä¸¤ç§å®ç°ï¼š

1. **Megatroné£æ ¼SP**ï¼ˆæœ€ä¸»æµï¼‰

```python
"""
Megatron Sequence Parallel:
åœ¨TPåŸºç¡€ä¸Šï¼Œå°†Dropout/LayerNormç­‰æ¿€æ´»åˆ‡åˆ†
ç”¨ReduceScatteræ›¿æ¢AllReduce
"""
# å‰å‘: ReduceScatter è¾“å‡ºæ¢¯åº¦ (åå‘æ—¶AllGather è¾“å…¥)
# æ¿€æ´»å†…å­˜: O(hidden / TP_size)
```

2. **Ulysses/Ringé£æ ¼SP**ï¼ˆé•¿åºåˆ—ä¼˜åŒ–ï¼‰

```python
"""
Ring Attentioné£æ ¼:
ä½¿ç”¨ç¯å½¢AllGatherå¤„ç†æ³¨æ„åŠ›çŸ©é˜µ
é¿å…å®Œæ•´Q@K^TçŸ©é˜µ
é€‚åˆè¶…é•¿åºåˆ— (>100K)
"""
```

#### Transformerä¸­çš„åº”ç”¨
æ³¨æ„åŠ›å±‚å¹¶è¡ŒåŒ–ï¼ˆSP + TPï¼‰

```python
class ParallelAttentionWithSP(nn.Module):
    def forward(self, hidden_states):
        # hidden_states: [batch, seq/TP_size, hidden] (å·²åˆ‡åˆ†)
        
        # QKVæŠ•å½± (TPåˆ—å¹¶è¡Œ)
        qkv = self.query_key_value(hidden_states)
        
        # Attentionè®¡ç®— (Ringæˆ–å±€éƒ¨)
        # åªè®¡ç®—å±€éƒ¨scoresï¼Œéœ€è¦Ring AllGather K/V
        
        # è¾“å‡ºæŠ•å½± (TPè¡Œå¹¶è¡Œ + ReduceScatter)
        output = self.dense(context)  # å·²åˆ‡åˆ†
        
        return output  # [batch, seq/TP_size, hidden]
```

MLP + LayerNorm

```python
# LayerNorm/Dropout: ç›´æ¥åœ¨åˆ‡åˆ†æ¿€æ´»ä¸Šè®¡ç®—ï¼Œæ— é¢å¤–é€šä¿¡
# MLP: ä¸TPç›¸åŒï¼Œä½†è¾“å…¥å·²åˆ‡åˆ†
```

#### é€šä¿¡ä¼˜åŒ–
- **é€šä¿¡ä¸è®¡ç®—é‡å **ï¼šå¼‚æ­¥AllGather/ReduceScatter
- **Ring Attention**ï¼šé¿å…O(seqÂ²)å†…å­˜ï¼Œé€šä¿¡é‡O(seq)
- **ä¸TPèåˆ**ï¼šå°†SPé€šä¿¡èå…¥TPçš„AllReduce

#### æ€§èƒ½åˆ†æ

| é…ç½®               | seq_len | TP Size | æ¿€æ´»å†…å­˜/GPU | MFU     |
|--------------------|---------|---------|--------------|---------|
| æ— SP               | 32K     | 8       | ~60GB        | 45%     |
| Megatron SP        | 128K    | 8       | ~15GB        | 58%     |
| Ring SP            | 1M      | 8       | ~8GB         | 55%     |

ç»“è®ºï¼š
- SPå¯å°†æ¿€æ´»å†…å­˜é™ä½~TP_sizeå€
- æ”¯æŒç™¾ä¸‡çº§åºåˆ—è®­ç»ƒ
- ä¸TPç»“åˆæ—¶é€šä¿¡å¼€é”€å¯æ§ï¼ˆ<15%ï¼‰

#### å®æˆ˜æ¡ˆä¾‹
- **Megatron-LM**ï¼šæœ€æ—©å®ç°SPï¼Œæ”¯æŒLlama-70Bé•¿ä¸Šä¸‹æ–‡è®­ç»ƒ
- **DeepSpeed-Ulysses**ï¼šUlysses SPå®ç°
- **Axolotl**ï¼šç¤¾åŒºå·¥å…·ï¼Œæ”¯æŒSPè®­ç»ƒé•¿ä¸Šä¸‹æ–‡Llama
- **å®é™…ç»éªŒ**ï¼šSPåº¦æ•°æ¨èä¸TPç›¸åŒï¼ˆ4-8ï¼‰ï¼Œé•¿åºåˆ—>64Kå¿…å¼€

