# æ–‡æ¡£ç´¢å¼•

æ¬¢è¿æ¥åˆ° MatrixDistributedComputing-5DParallel çš„æ–‡æ¡£ä¸­å¿ƒï¼

## ğŸ“– æ–‡æ¡£å¯¼èˆª

### å…¥é—¨æŒ‡å—

1. **[00-introduction.md](00-introduction.md)** - åˆ†å¸ƒå¼è®¡ç®—æ€»ä½“ä»‹ç»
   - ä¸ºä»€ä¹ˆéœ€è¦åˆ†å¸ƒå¼è®¡ç®—
   - åŸºç¡€æ¦‚å¿µå’Œæœ¯è¯­
   - é€šä¿¡åŸè¯­ä»‹ç»
   - ç¡¬ä»¶å’Œç½‘ç»œè¦æ±‚

2. **[setup-guide.md](setup-guide.md)** - ç¯å¢ƒé…ç½®æŒ‡å—
   - ç³»ç»Ÿè¦æ±‚
   - ä¾èµ–å®‰è£…
   - ç¯å¢ƒå˜é‡é…ç½®
   - éªŒè¯å®‰è£…

### 5Då¹¶è¡Œè¯¦è§£

3. **[01-data-parallelism.md](01-data-parallelism.md)** - æ•°æ®å¹¶è¡Œ (DP)
   - æ ¸å¿ƒåŸç†
   - DDP vs FSDP
   - å®ç°ç»†èŠ‚
   - æ€§èƒ½ä¼˜åŒ–

4. **[02-pipeline-parallelism.md](02-pipeline-parallelism.md)** - æµæ°´çº¿å¹¶è¡Œ (PP)
   - æµæ°´çº¿æ¶æ„
   - è°ƒåº¦ç­–ç•¥ (GPipe, PipeDream, 1F1B)
   - æ°”æ³¡é—®é¢˜
   - å®æˆ˜æ¡ˆä¾‹

5. **[03-tensor-parallelism.md](03-tensor-parallelism.md)** - å¼ é‡å¹¶è¡Œ (TP)
   - åˆ—å¹¶è¡Œä¸è¡Œå¹¶è¡Œ
   - Megatron-LMæ–¹æ³•
   - é€šä¿¡ä¼˜åŒ–
   - ä¸å…¶ä»–å¹¶è¡Œçš„ç»“åˆ

6. **[04-sequence-parallelism.md](04-sequence-parallelism.md)** - åºåˆ—å¹¶è¡Œ (SP)
   - åºåˆ—åˆ‡åˆ†ç­–ç•¥
   - æ¿€æ´»å†…å­˜ä¼˜åŒ–
   - Ring Attention
   - è¶…é•¿åºåˆ—è®­ç»ƒ

7. **[05-expert-parallelism.md](05-expert-parallelism.md)** - ä¸“å®¶å¹¶è¡Œ (EP)
   - MoEæ¶æ„
   - è·¯ç”±æœºåˆ¶
   - è´Ÿè½½å‡è¡¡
   - Switch Transformerå®ç°

8. **[06-hybrid-parallelism.md](06-hybrid-parallelism.md)** - æ··åˆå¹¶è¡Œç­–ç•¥
   - 3D/4D/5Då¹¶è¡Œ
   - å¹¶è¡Œåº¦é€‰æ‹©
   - é…ç½®ä¼˜åŒ–
   - å®é™…éƒ¨ç½²æ¡ˆä¾‹

### æ€§èƒ½ä¸æµ‹è¯•

9. **[benchmarks.md](benchmarks.md)** - æ€§èƒ½æµ‹è¯•å¯¹æ¯”
   - æµ‹è¯•æ–¹æ³•
   - æ€§èƒ½æŒ‡æ ‡
   - æ‰©å±•æ€§åˆ†æ
   - ä¸åŒç¡¬ä»¶é…ç½®çš„ç»“æœ

## ğŸ“š æ¨èé˜…è¯»é¡ºåº

### åˆå­¦è€…è·¯çº¿
```
00-introduction.md 
    â†“
setup-guide.md
    â†“
01-data-parallelism.md
    â†“
å®è·µï¼šè¿è¡Œ 01-data-parallelism/pytorch/dp_basic.py
```

### è¿›é˜¶å­¦ä¹ è·¯çº¿
```
02-pipeline-parallelism.md â†’ 03-tensor-parallelism.md
    â†“                              â†“
å®è·µï¼šPPç¤ºä¾‹                  å®è·µï¼šTPç¤ºä¾‹
    â†“                              â†“
04-sequence-parallelism.md â†’ 05-expert-parallelism.md
    â†“                              â†“
06-hybrid-parallelism.md (æ•´åˆæ‰€æœ‰çŸ¥è¯†)
    â†“
benchmarks.md (æ€§èƒ½åˆ†æ)
```

## ğŸ”— å¤–éƒ¨èµ„æº

### å®˜æ–¹æ–‡æ¡£
- [PyTorch Distributed](https://pytorch.org/tutorials/beginner/dist_overview.html)
- [DeepSpeed Documentation](https://www.deepspeed.ai/)
- [Megatron-LM GitHub](https://github.com/NVIDIA/Megatron-LM)
- [NCCL Documentation](https://docs.nvidia.com/deeplearning/nccl/)

### é‡è¦è®ºæ–‡
- [Megatron-LM (2019)](https://arxiv.org/abs/1909.08053)
- [ZeRO (2020)](https://arxiv.org/abs/1910.02054)
- [GPipe (2019)](https://arxiv.org/abs/1811.06965)
- [Switch Transformers (2021)](https://arxiv.org/abs/2101.03961)

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### æ–‡æ¡£ç»“æ„è¯´æ˜
æ¯ä¸ªæ–‡æ¡£éƒ½åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š
1. **æ¦‚è¿°**: å¿«é€Ÿäº†è§£æ ¸å¿ƒæ¦‚å¿µ
2. **åŸç†**: æ·±å…¥æŠ€æœ¯ç»†èŠ‚
3. **å®ç°**: å…·ä½“ä»£ç ç¤ºä¾‹
4. **ä¼˜åŒ–**: æ€§èƒ½è°ƒä¼˜æŠ€å·§
5. **å¸¸è§é—®é¢˜**: FAQå’Œæ•…éšœæ’æŸ¥

### ä»£ç ç¤ºä¾‹è¯´æ˜
- ğŸ“ **ç†è®ºè¯´æ˜**: æ–‡æ¡£ä¸­çš„å›¾è¡¨å’Œä¼ªä»£ç 
- ğŸ’» **å¯è¿è¡Œä»£ç **: å¯¹åº”æ–‡ä»¶å¤¹ä¸­çš„å®Œæ•´å®ç°
- ğŸ§ª **æµ‹è¯•ç”¨ä¾‹**: tests/ ç›®å½•ä¸­çš„å•å…ƒæµ‹è¯•

## ğŸ› ï¸ æ–‡æ¡£è´¡çŒ®

å‘ç°æ–‡æ¡£é—®é¢˜æˆ–æƒ³è¦æ”¹è¿›ï¼Ÿ
1. åœ¨ [Issues](../../issues) ä¸­æŠ¥å‘Šé—®é¢˜
2. æäº¤ Pull Request æ”¹è¿›æ–‡æ¡£
3. åˆ†äº«ä½ çš„å®è·µç»éªŒ

## ğŸ“Š æ–‡æ¡£æ›´æ–°æ—¥å¿—

- **2024-12**: åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
  - å®Œæˆ5Då¹¶è¡Œæ‰€æœ‰æ–‡æ¡£
  - æ·»åŠ ä»£ç ç¤ºä¾‹
  - æ€§èƒ½æµ‹è¯•ç»“æœ

---

<div align="center">
  <strong>Happy Learning! ğŸš€</strong>
</div>