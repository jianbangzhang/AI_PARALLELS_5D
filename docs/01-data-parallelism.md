# æ•°æ®å¹¶è¡Œ (Data Parallelism)

## ğŸ“‹ ç›®å½•

- [æ ¸å¿ƒåŸç†](#æ ¸å¿ƒåŸç†)
- [å®ç°æ–¹å¼](#å®ç°æ–¹å¼)
- [DDPè¯¦è§£](#ddpè¯¦è§£)
- [FSDPè¯¦è§£](#fsdpè¯¦è§£)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [å®æˆ˜æ¡ˆä¾‹](#å®æˆ˜æ¡ˆä¾‹)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## æ ¸å¿ƒåŸç†

### ä»€ä¹ˆæ˜¯æ•°æ®å¹¶è¡Œï¼Ÿ

æ•°æ®å¹¶è¡Œæ˜¯**æœ€ç®€å•ã€æœ€å¸¸ç”¨**çš„å¹¶è¡Œç­–ç•¥ï¼š

```
åŸºæœ¬æ€æƒ³:
1. æ¯ä¸ªGPUæŒæœ‰å®Œæ•´æ¨¡å‹çš„å‰¯æœ¬
2. å°†è®­ç»ƒæ•°æ®åˆ†æˆå¤šä¸ªbatch
3. æ¯ä¸ªGPUå¤„ç†ä¸åŒçš„æ•°æ®batch
4. é€šè¿‡AllReduceåŒæ­¥æ¢¯åº¦
5. æ‰€æœ‰GPUä½¿ç”¨ç›¸åŒçš„æ¢¯åº¦æ›´æ–°æ¨¡å‹
```

### å·¥ä½œæµç¨‹

```python
"""
æ•°æ®å¹¶è¡Œè®­ç»ƒæµç¨‹ (4ä¸ªGPUç¤ºä¾‹)
"""

# æ­¥éª¤1: åˆå§‹åŒ– - æ¯ä¸ªGPUå¤åˆ¶å®Œæ•´æ¨¡å‹
GPU 0: Model (Wâ‚€ = W_init)
GPU 1: Model (Wâ‚ = W_init)  # ä¸GPU 0ç›¸åŒ
GPU 2: Model (Wâ‚‚ = W_init)  # ä¸GPU 0ç›¸åŒ
GPU 3: Model (Wâ‚ƒ = W_init)  # ä¸GPU 0ç›¸åŒ

# æ­¥éª¤2: æ•°æ®åˆ†å‘ - ä¸åŒGPUå¤„ç†ä¸åŒæ•°æ®
GPU 0: Data batch [0-31]
GPU 1: Data batch [32-63]
GPU 2: Data batch [64-95]
GPU 3: Data batch [96-127]

# æ­¥éª¤3: å‰å‘ä¼ æ’­ - å„è‡ªç‹¬ç«‹è®¡ç®—
GPU 0: lossâ‚€ = forward(Data[0-31])
GPU 1: lossâ‚ = forward(Data[32-63])
GPU 2: lossâ‚‚ = forward(Data[64-95])
GPU 3: lossâ‚ƒ = forward(Data[96-127])

# æ­¥éª¤4: åå‘ä¼ æ’­ - å„è‡ªç‹¬ç«‹è®¡ç®—æ¢¯åº¦
GPU 0: gradâ‚€ = backward(lossâ‚€)
GPU 1: gradâ‚ = backward(lossâ‚)
GPU 2: gradâ‚‚ = backward(lossâ‚‚)
GPU 3: gradâ‚ƒ = backward(lossâ‚ƒ)

# æ­¥éª¤5: æ¢¯åº¦åŒæ­¥ - AllReduceæ±‚å¹³å‡
AllReduce(gradâ‚€, gradâ‚, gradâ‚‚, gradâ‚ƒ)
â†’ grad_avg = (gradâ‚€ + gradâ‚ + gradâ‚‚ + gradâ‚ƒ) / 4

# æ­¥éª¤6: å‚æ•°æ›´æ–° - æ‰€æœ‰GPUä½¿ç”¨ç›¸åŒæ¢¯åº¦
GPU 0: Wâ‚€ = Wâ‚€ - lr Ã— grad_avg
GPU 1: Wâ‚ = Wâ‚ - lr Ã— grad_avg
GPU 2: Wâ‚‚ = Wâ‚‚ - lr Ã— grad_avg
GPU 3: Wâ‚ƒ = Wâ‚ƒ - lr Ã— grad_avg

# ç»“æœ: æ‰€æœ‰GPUçš„æ¨¡å‹å‚æ•°ä¿æŒåŒæ­¥
```

### å…³é”®ç‰¹ç‚¹

| ç‰¹æ€§ | è¯´æ˜ |
|-----|------|
| âœ… **ç®€å•ç›´è§‚** | æœ€å®¹æ˜“ç†è§£å’Œå®ç°çš„å¹¶è¡Œæ–¹å¼ |
| âœ… **é«˜æ•ˆç‡** | é€šä¿¡å¼€é”€ç›¸å¯¹è¾ƒå° |
| âœ… **çº¿æ€§åŠ é€Ÿ** | ç†æƒ³æƒ…å†µä¸‹å¯è¾¾åˆ°è¿‘çº¿æ€§åŠ é€Ÿ |
| âŒ **å†…å­˜é™åˆ¶** | æ¯ä¸ªGPUå¿…é¡»èƒ½è£…ä¸‹å®Œæ•´æ¨¡å‹ |
| âŒ **é€šä¿¡ç“¶é¢ˆ** | å¤§æ¨¡å‹æ¢¯åº¦åŒæ­¥å¼€é”€å¤§ |

---

## å®ç°æ–¹å¼

### 1. DataParallel (DP) - å•æœºå¤šå¡

```python
import torch
import torch.nn as nn

# ç®€å•åŒ…è£…å³å¯
model = nn.Linear(1000, 1000)
if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model)
model = model.cuda()

# è®­ç»ƒ
data = torch.randn(128, 1000).cuda()
output = model(data)
```

**ç‰¹ç‚¹**:
- âœ… æœ€ç®€å•ï¼Œä¸€è¡Œä»£ç å¯ç”¨
- âŒ å•è¿›ç¨‹å¤šçº¿ç¨‹ï¼ŒGILé™åˆ¶
- âŒ GPU 0è´Ÿè½½é‡ï¼ˆå‚æ•°æœåŠ¡å™¨ï¼‰
- âŒ ä¸æ”¯æŒå¤šèŠ‚ç‚¹
- âš ï¸ **å·²ä¸æ¨èä½¿ç”¨**ï¼Œå»ºè®®ç”¨DDP

### 2. DistributedDataParallel (DDP) - æ¨è

```python
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# åˆå§‹åŒ–è¿›ç¨‹ç»„
dist.init_process_group(backend='nccl')
rank = dist.get_rank()

# åˆ›å»ºæ¨¡å‹å¹¶ç§»åˆ°å¯¹åº”GPU
model = MyModel().cuda(rank)
model = DDP(model, device_ids=[rank])

# è®­ç»ƒ
for data, target in dataloader:
    output = model(data)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()
```

**ç‰¹ç‚¹**:
- âœ… å¤šè¿›ç¨‹ï¼Œæ— GILé™åˆ¶
- âœ… æ”¯æŒå¤šèŠ‚ç‚¹è®­ç»ƒ
- âœ… é€šä¿¡é«˜æ•ˆï¼ˆRing AllReduceï¼‰
- âœ… è´Ÿè½½å‡è¡¡
- âœ… ç¤¾åŒºæ ‡å‡†ï¼Œç”Ÿäº§çº§

### 3. Fully Sharded Data Parallel (FSDP) - æœ€æ–°

```python
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP

# åˆ›å»ºæ¨¡å‹
model = MyLargeModel()

# FSDPåŒ…è£…
model = FSDP(
    model,
    auto_wrap_policy=my_auto_wrap_policy,
    mixed_precision=mixed_precision_policy,
)

# è®­ç»ƒï¼ˆä¸DDPç›¸åŒï¼‰
for data, target in dataloader:
    output = model(data)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()
```

**ç‰¹ç‚¹**:
- âœ… ZeRO-3é£æ ¼åˆ†ç‰‡
- âœ… å¤§å¹…å‡å°‘å†…å­˜å ç”¨
- âœ… æ”¯æŒè¶…å¤§æ¨¡å‹
- âœ… PyTorchåŸç”Ÿæ”¯æŒ
- âš ï¸ é€šä¿¡å¼€é”€ç•¥é«˜äºDDP

---

## DDPè¯¦è§£

### æ¶æ„è®¾è®¡

```
DDPæ¶æ„:

è¿›ç¨‹0 (GPU 0)          è¿›ç¨‹1 (GPU 1)          è¿›ç¨‹2 (GPU 2)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Modelå‰¯æœ¬   â”‚      â”‚  Modelå‰¯æœ¬   â”‚      â”‚  Modelå‰¯æœ¬   â”‚
â”‚   + Grad     â”‚      â”‚   + Grad     â”‚      â”‚   + Grad     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                     â”‚                     â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  AllReduceé€šä¿¡   â”‚
                    â”‚   (NCCL/Gloo)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å®Œæ•´ä»£ç ç¤ºä¾‹

```python
"""
å®Œæ•´çš„DDPè®­ç»ƒè„šæœ¬
è¿è¡Œ: torchrun --nproc_per_node=4 train_ddp.py
"""

import os
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler


def setup(rank, world_size):
    """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
    # ç¯å¢ƒå˜é‡ç”±torchrunè‡ªåŠ¨è®¾ç½®
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    
    # åˆå§‹åŒ–è¿›ç¨‹ç»„
    dist.init_process_group(
        backend='nccl',  # GPUç”¨ncclï¼ŒCPUç”¨gloo
        rank=rank,
        world_size=world_size
    )
    
    # è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„GPU
    torch.cuda.set_device(rank)


def cleanup():
    """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
    dist.destroy_process_group()


class ToyModel(nn.Module):
    """ç¤ºä¾‹æ¨¡å‹"""
    def __init__(self):
        super().__init__()
        self.net1 = nn.Linear(10, 10)
        self.relu = nn.ReLU()
        self.net2 = nn.Linear(10, 5)
    
    def forward(self, x):
        return self.net2(self.relu(self.net1(x)))


def train_ddp(rank, world_size):
    """DDPè®­ç»ƒå‡½æ•°"""
    print(f"Running DDP on rank {rank}.")
    setup(rank, world_size)
    
    # åˆ›å»ºæ¨¡å‹å¹¶ç§»åˆ°GPU
    model = ToyModel().to(rank)
    
    # DDPåŒ…è£…
    ddp_model = DDP(model, device_ids=[rank])
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.SGD(ddp_model.parameters(), lr=0.001)
    loss_fn = nn.MSELoss()
    
    # æ•°æ®é›†å’ŒDataLoader
    # é‡è¦ï¼šä½¿ç”¨DistributedSamplerç¡®ä¿æ¯ä¸ªè¿›ç¨‹çœ‹åˆ°ä¸åŒæ•°æ®
    dataset = torch.randn(1000, 10)  # ç¤ºä¾‹æ•°æ®
    sampler = DistributedSampler(
        dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=True
    )
    dataloader = DataLoader(
        dataset,
        batch_size=32,
        sampler=sampler
    )
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(10):
        # è®¾ç½®epochä»¥shuffleæ•°æ®
        sampler.set_epoch(epoch)
        
        for batch_idx, data in enumerate(dataloader):
            data = data.to(rank)
            targets = torch.randn(data.size(0), 5).to(rank)
            
            # å‰å‘ä¼ æ’­
            outputs = ddp_model(data)
            loss = loss_fn(outputs, targets)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            
            # DDPè‡ªåŠ¨å¤„ç†æ¢¯åº¦åŒæ­¥
            optimizer.step()
            
            if rank == 0 and batch_idx % 10 == 0:
                print(f"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}")
    
    cleanup()


if __name__ == "__main__":
    world_size = torch.cuda.device_count()
    
    # ä½¿ç”¨torch.multiprocessingå¯åŠ¨å¤šè¿›ç¨‹
    import torch.multiprocessing as mp
    mp.spawn(
        train_ddp,
        args=(world_size,),
        nprocs=world_size,
        join=True
    )
```

### DDPé€šä¿¡æœºåˆ¶

#### Gradient Bucketing

```python
"""
DDPå°†æ¢¯åº¦åˆ†ç»„æˆbucketsè¿›è¡Œé€šä¿¡
"""

# é»˜è®¤bucketå¤§å°: 25 MB
model = DDP(
    model,
    device_ids=[rank],
    bucket_cap_mb=25  # å¯è°ƒæ•´
)

# é€šä¿¡æµç¨‹:
# 1. åå‘ä¼ æ’­å¼€å§‹
# 2. å½“ä¸€ä¸ªbucketçš„æ‰€æœ‰æ¢¯åº¦readyæ—¶
# 3. ç«‹å³å¯åŠ¨AllReduce (ä¸ç­‰æ‰€æœ‰æ¢¯åº¦)
# 4. å®ç°è®¡ç®—å’Œé€šä¿¡é‡å 
```

**å¯è§†åŒ–**:
```
æ—¶é—´çº¿:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Backward Pass:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                  â†“   â†“   â†“   â†“   â†“   â†“
Bucket 0 Ready:   â– 
AllReduce 0:      â•â•â•â•â•â•â•â•—
Bucket 1 Ready:       â–    â•‘
AllReduce 1:          â•â•â•â•â•â•â•â•—
Bucket 2 Ready:           â–    â•‘
AllReduce 2:              â•â•â•â•â•â•â•â•—
                                  â•‘
è®¡ç®—å’Œé€šä¿¡é‡å :     â–ˆâ–ˆâ–ˆâ–ˆâ–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â•‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

#### Gradient Accumulation

```python
"""
æ¢¯åº¦ç´¯ç§¯ - æ¨¡æ‹Ÿæ›´å¤§çš„batch size
"""

accumulation_steps = 4  # ç´¯ç§¯4ä¸ªstep
optimizer.zero_grad()

for i, (data, target) in enumerate(dataloader):
    output = model(data)
    loss = criterion(output, target)
    
    # å½’ä¸€åŒ–loss
    loss = loss / accumulation_steps
    loss.backward()
    
    # æ¯accumulation_stepsä¸ªstepæ›´æ–°ä¸€æ¬¡
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# ä¼˜ç‚¹:
# 1. å‡å°‘é€šä¿¡é¢‘ç‡ (4å€)
# 2. æ¨¡æ‹Ÿæ›´å¤§batch size
# 3. èŠ‚çœæ˜¾å­˜
```

### æ··åˆç²¾åº¦è®­ç»ƒ

```python
"""
ä½¿ç”¨AMP (Automatic Mixed Precision)
"""

from torch.cuda.amp import autocast, GradScaler

model = DDP(model, device_ids=[rank])
optimizer = torch.optim.Adam(model.parameters())
scaler = GradScaler()

for data, target in dataloader:
    optimizer.zero_grad()
    
    # è‡ªåŠ¨æ··åˆç²¾åº¦
    with autocast():
        output = model(data)
        loss = criterion(output, target)
    
    # ç¼©æ”¾losså¹¶åå‘ä¼ æ’­
    scaler.scale(loss).backward()
    
    # æ¢¯åº¦è£å‰ªï¼ˆå¯é€‰ï¼‰
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    
    # æ›´æ–°å‚æ•°
    scaler.step(optimizer)
    scaler.update()

# å¥½å¤„:
# - 2å€åŠ é€Ÿ
# - å‡å°‘50%æ˜¾å­˜
# - å‡ ä¹æ— ç²¾åº¦æŸå¤±
```

---

## FSDPè¯¦è§£

### ZeROåŸç†

FSDPå®ç°äº†ZeRO (Zero Redundancy Optimizer) Stage 3:

```
ä¼ ç»ŸDDPå†…å­˜å ç”¨:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPU 0: å‚æ•° + æ¢¯åº¦ + ä¼˜åŒ–å™¨çŠ¶æ€     â”‚ 100%
â”‚ GPU 1: å‚æ•° + æ¢¯åº¦ + ä¼˜åŒ–å™¨çŠ¶æ€     â”‚ 100%
â”‚ GPU 2: å‚æ•° + æ¢¯åº¦ + ä¼˜åŒ–å™¨çŠ¶æ€     â”‚ 100%
â”‚ GPU 3: å‚æ•° + æ¢¯åº¦ + ä¼˜åŒ–å™¨çŠ¶æ€     â”‚ 100%
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ZeRO-3 (FSDP)å†…å­˜å ç”¨:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPU 0: å‚æ•°â‚€ + æ¢¯åº¦â‚€ + ä¼˜åŒ–å™¨â‚€      â”‚ 25%
â”‚ GPU 1: å‚æ•°â‚ + æ¢¯åº¦â‚ + ä¼˜åŒ–å™¨â‚      â”‚ 25%
â”‚ GPU 2: å‚æ•°â‚‚ + æ¢¯åº¦â‚‚ + ä¼˜åŒ–å™¨â‚‚      â”‚ 25%
â”‚ GPU 3: å‚æ•°â‚ƒ + æ¢¯åº¦â‚ƒ + ä¼˜åŒ–å™¨â‚ƒ      â”‚ 25%
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å†…å­˜èŠ‚çœ: 4Ã— (å¯¹äº4ä¸ªGPU)
```

### FSDPå·¥ä½œæµç¨‹

```python
"""
FSDPå‰å‘ä¼ æ’­
"""

# æ­¥éª¤1: All-Gatherå‚æ•°
for layer in model:
    # æ”¶é›†æ‰€æœ‰GPUçš„å‚æ•°åˆ†ç‰‡
    full_params = all_gather(layer.params_shard)
    
    # æ­¥éª¤2: å‰å‘è®¡ç®—
    output = layer.forward(input, full_params)
    
    # æ­¥éª¤3: é‡Šæ”¾å®Œæ•´å‚æ•°ï¼ˆä¿ç•™åˆ†ç‰‡ï¼‰
    del full_params

"""
FSDPåå‘ä¼ æ’­
"""

# æ­¥éª¤1: All-Gatherå‚æ•°
for layer in reversed(model):
    full_params = all_gather(layer.params_shard)
    
    # æ­¥éª¤2: åå‘è®¡ç®—
    grad = layer.backward(grad_output, full_params)
    
    # æ­¥éª¤3: Reduce-Scatteræ¢¯åº¦
    layer.grad_shard = reduce_scatter(grad)
    
    # æ­¥éª¤4: é‡Šæ”¾å®Œæ•´å‚æ•°å’Œæ¢¯åº¦
    del full_params, grad
```

### FSDPå®Œæ•´ç¤ºä¾‹

```python
"""
FSDPè®­ç»ƒç¤ºä¾‹
"""

import torch
import torch.nn as nn
from torch.distributed.fsdp import (
    FullyShardedDataParallel as FSDP,
    MixedPrecision,
    BackwardPrefetch,
    ShardingStrategy,
)
from torch.distributed.fsdp.wrap import (
    size_based_auto_wrap_policy,
    enable_wrap,
    wrap,
)


def setup_fsdp():
    """é…ç½®FSDPç­–ç•¥"""
    
    # æ··åˆç²¾åº¦ç­–ç•¥
    mixed_precision_policy = MixedPrecision(
        param_dtype=torch.float16,      # å‚æ•°ç”¨FP16
        reduce_dtype=torch.float16,     # æ¢¯åº¦reduceç”¨FP16
        buffer_dtype=torch.float32,     # bufferç”¨FP32
    )
    
    # Shardingç­–ç•¥
    sharding_strategy = ShardingStrategy.FULL_SHARD  # ZeRO-3
    # å…¶ä»–é€‰é¡¹:
    # - SHARD_GRAD_OP: ZeRO-2 (åˆ†ç‰‡æ¢¯åº¦å’Œä¼˜åŒ–å™¨)
    # - NO_SHARD: ç±»ä¼¼DDP
    # - HYBRID_SHARD: èŠ‚ç‚¹å†…å…¨åˆ†ç‰‡ï¼ŒèŠ‚ç‚¹é—´å¤åˆ¶
    
    return mixed_precision_policy, sharding_strategy


class TransformerBlock(nn.Module):
    """Transformerå—"""
    def __init__(self, dim, num_heads):
        super().__init__()
        self.attention = nn.MultiheadAttention(dim, num_heads)
        self.ff = nn.Sequential(
            nn.Linear(dim, 4 * dim),
            nn.GELU(),
            nn.Linear(4 * dim, dim),
        )
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)
    
    def forward(self, x):
        # Attention
        attn_out, _ = self.attention(x, x, x)
        x = self.norm1(x + attn_out)
        
        # Feedforward
        ff_out = self.ff(x)
        x = self.norm2(x + ff_out)
        
        return x


def build_model():
    """æ„å»ºå¤§æ¨¡å‹"""
    dim = 2048
    num_layers = 48
    num_heads = 32
    
    # ä½¿ç”¨enable_wrapè‡ªåŠ¨åŒ…è£…
    mixed_precision, sharding_strategy = setup_fsdp()
    
    # è‡ªåŠ¨åŒ…è£…ç­–ç•¥: æ ¹æ®å‚æ•°é‡å†³å®š
    auto_wrap_policy = size_based_auto_wrap_policy(
        min_num_params=1e8  # 100Må‚æ•°ä»¥ä¸Šçš„æ¨¡å—ç‹¬ç«‹åˆ†ç‰‡
    )
    
    with enable_wrap(
        wrapper_cls=FSDP,
        mixed_precision=mixed_precision,
        sharding_strategy=sharding_strategy,
        auto_wrap_policy=auto_wrap_policy,
    ):
        model = nn.Sequential(
            nn.Embedding(50000, dim),
            *[wrap(TransformerBlock(dim, num_heads)) for _ in range(num_layers)],
            nn.Linear(dim, 50000)
        )
    
    # æ•´ä¸ªæ¨¡å‹æœ€å¤–å±‚å†åŒ…è£…ä¸€æ¬¡
    model = FSDP(
        model,
        mixed_precision=mixed_precision,
        sharding_strategy=sharding_strategy,
        backward_prefetch=BackwardPrefetch.BACKWARD_PRE,  # é¢„å–ä¼˜åŒ–
    )
    
    return model


def train_fsdp():
    """è®­ç»ƒ"""
    dist.init_process_group(backend='nccl')
    rank = dist.get_rank()
    
    # åˆ›å»ºæ¨¡å‹
    model = build_model().cuda(rank)
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    
    # è®­ç»ƒ
    for epoch in range(10):
        for data, target in dataloader:
            data, target = data.cuda(rank), target.cuda(rank)
            
            output = model(data)
            loss = F.cross_entropy(output, target)
            
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            if rank == 0:
                print(f"Loss: {loss.item():.4f}")
    
    dist.destroy_process_group()
```

### FSDP vs DDP å¯¹æ¯”

| ç‰¹æ€§ | DDP | FSDP |
|-----|-----|------|
| å†…å­˜æ•ˆç‡ | ä½ (æ¯GPUå®Œæ•´æ¨¡å‹) | é«˜ (åˆ†ç‰‡) |
| é€šä¿¡é‡ | ä¸­ç­‰ | è¾ƒé«˜ |
| å®ç°å¤æ‚åº¦ | ç®€å• | ä¸­ç­‰ |
| æœ€å¤§æ¨¡å‹ | ~10B (8Ã—A100) | ~100B+ |
| è®­ç»ƒé€Ÿåº¦ | å¿« | ç•¥æ…¢10-20% |
| é€‚ç”¨åœºæ™¯ | å°ä¸­å‹æ¨¡å‹ | è¶…å¤§æ¨¡å‹ |

---

## æ€§èƒ½ä¼˜åŒ–

### 1. é€šä¿¡ä¼˜åŒ–

#### ä½¿ç”¨æ··åˆç²¾åº¦

```python
# FP16å¯ä»¥å‡å°‘2å€é€šä¿¡é‡
scaler = GradScaler()

with autocast():
    output = model(data)
    loss = criterion(output, target)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

#### æ¢¯åº¦å‹ç¼©

```python
# PowerSGDæ¢¯åº¦å‹ç¼©
from torch.distributed.algorithms.ddp_comm_hooks import powerSGD_hook

model = DDP(model, device_ids=[rank])

# æ³¨å†Œé€šä¿¡hook
state = powerSGD_hook.PowerSGDState(
    process_group=None,
    matrix_approximation_rank=4,  # å‹ç¼©ç§©
    start_powerSGD_iter=10,  # ä»ç¬¬10æ¬¡è¿­ä»£å¼€å§‹å‹ç¼©
)
model.register_comm_hook(state, powerSGD_hook.powerSGD_hook)

# å¯ä»¥å‡å°‘5-10å€é€šä¿¡é‡
```

### 2. è®¡ç®—ä¼˜åŒ–

#### ä½¿ç”¨ç¼–è¯‘å™¨ä¼˜åŒ–

```python
# PyTorch 2.0+ æ”¯æŒtorch.compile
model = torch.compile(model, mode="reduce-overhead")
model = DDP(model, device_ids=[rank])

# å¯ä»¥åŠ é€Ÿ10-30%
```

#### CUDA Graphs

```python
# å¯¹äºå›ºå®šè¾“å…¥shapeçš„æ¨¡å‹
use_cuda_graph = True
if use_cuda_graph:
    # Warmup
    for _ in range(10):
        output = model(sample_input)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
    
    # Capture
    g = torch.cuda.CUDAGraph()
    optimizer.zero_grad()
    with torch.cuda.graph(g):
        static_output = model(static_input)
        static_loss = criterion(static_output, static_target)
        static_loss.backward()
    
    # Replay
    for data, target in dataloader:
        static_input.copy_(data)
        static_target.copy_(target)
        g.replay()
        optimizer.step()
        optimizer.zero_grad()
```

### 3. æ•°æ®åŠ è½½ä¼˜åŒ–

```python
dataloader = DataLoader(
    dataset,
    batch_size=batch_size,
    num_workers=4,          # å¤šè¿›ç¨‹åŠ è½½
    pin_memory=True,        # é”é¡µå†…å­˜
    prefetch_factor=2,      # é¢„å–
    persistent_workers=True # ä¿æŒworkerè¿›ç¨‹
)
```

---

## å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1: GPT-2è®­ç»ƒ

```python
"""
ä½¿ç”¨DDPè®­ç»ƒGPT-2 (1.5Bå‚æ•°)
"""

from transformers import GPT2LMHeadModel, GPT2Tokenizer

def train_gpt2():
    dist.init_process_group(backend='nccl')
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    
    # åŠ è½½æ¨¡å‹
    model = GPT2LMHeadModel.from_pretrained('gpt2-large')
    model = model.cuda(rank)
    model = DDP(model, device_ids=[rank])
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
    
    # æ•°æ®
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')
    # ... æ•°æ®åŠ è½½ä»£ç  ...
    
    # è®­ç»ƒ
    model.train()
    for epoch in range(epochs):
        for batch in dataloader:
            input_ids = batch['input_ids'].cuda(rank)
            labels = input_ids.clone()
            
            outputs = model(input_ids=input_ids, labels=labels)
            loss = outputs.loss
            
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
    
    # ä¿å­˜æ¨¡å‹ (åªåœ¨rank 0ä¿å­˜)
    if rank == 0:
        model.module.save_pretrained('./gpt2-finetuned')
```

### æ¡ˆä¾‹2: è¶…å‚æ•°æœç´¢

```python
"""
å¹¶è¡Œè¶…å‚æ•°æœç´¢
"""

def hyperparameter_search():
    # æ¯ä¸ªGPUæµ‹è¯•ä¸åŒçš„è¶…å‚æ•°
    rank = dist.get_rank()
    
    # è¶…å‚æ•°ç½‘æ ¼
    learning_rates = [1e-5, 5e-5, 1e-4, 5e-4]
    lr = learning_rates[rank]
    
    # è®­ç»ƒ
    model = create_model()
    model = DDP(model, device_ids=[rank])
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    
    val_loss = train(model, optimizer)
    
    # æ”¶é›†æ‰€æœ‰ç»“æœ
    all_results = [None] * dist.get_world_size()
    dist.all_gather_object(all_results, {'lr': lr, 'val_loss': val_loss})
    
    if rank == 0:
        best = min(all_results, key=lambda x: x['val_loss'])
        print(f"Best LR: {best['lr']}, Loss: {best['val_loss']}")
```

---

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆDDPæ¯”DPå¿«ï¼Ÿ

**A**: å¤šè¿›ç¨‹ vs å•è¿›ç¨‹å¤šçº¿ç¨‹
- DDP: æ¯ä¸ªGPUç‹¬ç«‹çš„Pythonè¿›ç¨‹ï¼Œæ— GIL
- DP: å•è¿›ç¨‹ï¼Œå—Python GILé™åˆ¶

### Q2: Batch Sizeå¦‚ä½•è®¾ç½®ï¼Ÿ

**A**: å…¨å±€Batch Size = local_batch_size Ã— world_size

```python
# ç¤ºä¾‹: 4 GPUs, å…¨å±€BS=128
local_batch_size = 128 // 4  # = 32
global_batch_size = local_batch_size * 4  # = 128
```

### Q3: å¦‚ä½•ä¿å­˜å’ŒåŠ è½½æ¨¡å‹ï¼Ÿ

```python
# ä¿å­˜ (åªåœ¨rank 0)
if rank == 0:
    torch.save(model.module.state_dict(), 'model.pt')

# åŠ è½½ (æ‰€æœ‰rank)
dist.barrier()  # ç­‰å¾…ä¿å­˜å®Œæˆ
model.module.load_state_dict(torch.load('model.pt'))
```

### Q4: OOMæ€ä¹ˆåŠï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**:
1. å‡å°local batch size
2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
3. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
4. ä½¿ç”¨FSDP
5. ä½¿ç”¨æ··åˆç²¾åº¦

```python
# æ¢¯åº¦æ£€æŸ¥ç‚¹ç¤ºä¾‹
from torch.utils.checkpoint import checkpoint

class MyModel(nn.Module):
    def forward(self, x):
        # å¯¹å¤§å±‚ä½¿ç”¨checkpoint
        x = checkpoint(self.big_layer, x)
        return x
```

### Q5: å¤šèŠ‚ç‚¹å¦‚ä½•è¿è¡Œï¼Ÿ

```bash
# èŠ‚ç‚¹0 (master)
torchrun \
    --nnodes=2 \
    --node_rank=0 \
    --master_addr=192.168.1.1 \
    --master_port=29500 \
    --nproc_per_node=8 \
    train.py

# èŠ‚ç‚¹1
torchrun \
    --nnodes=2 \
    --node_rank=1 \
    --master_addr=192.168.1.1 \
    --master_port=29500 \
    --nproc_per_node=8 \
    train.py
```

---

## æ€»ç»“

### æ•°æ®å¹¶è¡Œé€‰æ‹©æŒ‡å—

```
å†³ç­–æ ‘:

æ¨¡å‹èƒ½æ”¾å…¥å•GPU? 
â”œâ”€ Yes â†’ ä½¿ç”¨DDP (ç®€å•é«˜æ•ˆ)
â””â”€ No â†’ ç»§ç»­åˆ¤æ–­
    
    æ¨¡å‹ < 50B?
    â”œâ”€ Yes â†’ ä½¿ç”¨FSDP (ZeRO-3)
    â””â”€ No â†’ è€ƒè™‘æ¨¡å‹å¹¶è¡Œ (TP/PP)
```

### æœ€ä½³å®è·µ

âœ… é¦–é€‰DDP - é€‚åˆ90%çš„åœºæ™¯
âœ… å¤§æ¨¡å‹ç”¨FSDP - å‚æ•°>10Bæ—¶
âœ… å¯ç”¨æ··åˆç²¾åº¦ - å…è´¹2å€åŠ é€Ÿ
âœ… åˆç†è®¾ç½®batch size - ä¿è¯GPUåˆ©ç”¨ç‡>80%
âœ… ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ - æ¨¡æ‹Ÿæ›´å¤§batch
âœ… ç›‘æ§é€šä¿¡å¼€é”€ - åº”<20%è®­ç»ƒæ—¶é—´