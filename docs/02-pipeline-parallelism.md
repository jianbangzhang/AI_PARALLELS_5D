# æµæ°´çº¿å¹¶è¡Œ (Pipeline Parallelism)

## ğŸ“‹ ç›®å½•

- [æ ¸å¿ƒåŸç†](#æ ¸å¿ƒåŸç†)
- [è°ƒåº¦ç­–ç•¥](#è°ƒåº¦ç­–ç•¥)
- [GPipeè¯¦è§£](#gpipeè¯¦è§£)
- [PipeDreamè¯¦è§£](#pipedreamè¯¦è§£)
- [1F1Bè°ƒåº¦](#1f1bè°ƒåº¦)
- [å®ç°ç»†èŠ‚](#å®ç°ç»†èŠ‚)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [å®æˆ˜æ¡ˆä¾‹](#å®æˆ˜æ¡ˆä¾‹)

---

## æ ¸å¿ƒåŸç†

### ä»€ä¹ˆæ˜¯æµæ°´çº¿å¹¶è¡Œï¼Ÿ

```
åŸºæœ¬æ€æƒ³:
å°†æ·±åº¦ç¥ç»ç½‘ç»œæŒ‰å±‚åˆ‡åˆ†æˆå¤šä¸ªé˜¶æ®µ(Stage)
æ¯ä¸ªé˜¶æ®µåœ¨ä¸åŒçš„è®¾å¤‡ä¸Šæ‰§è¡Œ
æ•°æ®åƒæµæ°´çº¿ä¸€æ ·åœ¨è®¾å¤‡é—´ä¼ é€’
```

### ä¸ºä»€ä¹ˆéœ€è¦æµæ°´çº¿å¹¶è¡Œï¼Ÿ

**é—®é¢˜**: æ¨¡å‹å¤ªæ·±ï¼Œå•ä¸ªGPUè£…ä¸ä¸‹

```python
# ä¾‹å¦‚: GPT-3 (96å±‚ Transformer)
Model:
    Embedding Layer
    â”œâ”€ Transformer Block 1-24   â†’ GPU 0
    â”œâ”€ Transformer Block 25-48  â†’ GPU 1  
    â”œâ”€ Transformer Block 49-72  â†’ GPU 2
    â””â”€ Transformer Block 73-96  â†’ GPU 3
    Output Layer

å•ä¸ªGPUå†…å­˜: ä¸å¤Ÿï¼
è§£å†³æ–¹æ¡ˆ: æµæ°´çº¿å¹¶è¡Œ
```

### æœ´ç´ æµæ°´çº¿çš„é—®é¢˜

```
æ—¶é—´çº¿ (4ä¸ªé˜¶æ®µï¼Œ1ä¸ªbatch):

Stage 0:  [F0]â”€â”€â”€â”€â”€â”€â”€â”€â”€[B0]â”€â”€â”€â”€â”€â”€â”€â”€â”€
Stage 1:  â”€â”€â”€â”€[F0]â”€â”€â”€â”€â”€â”€â”€â”€â”€[B0]â”€â”€â”€â”€â”€
Stage 2:  â”€â”€â”€â”€â”€â”€â”€â”€[F0]â”€â”€â”€â”€â”€â”€â”€â”€â”€[B0]â”€
Stage 3:  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[F0]â”€â”€â”€â”€â”€â”€â”€â”€â”€[B0]

F = Forward, B = Backward
é—®é¢˜: å¤§é‡ç©ºé—²æ—¶é—´(æ°”æ³¡) â‰ˆ 75%
```

---

## è°ƒåº¦ç­–ç•¥

### 1. GPipe - å¾®æ‰¹æ¬¡æµæ°´çº¿

**æ ¸å¿ƒæ€æƒ³**: å°†batchåˆ‡åˆ†æˆå¤šä¸ªmicro-batchï¼Œå¡«å……æµæ°´çº¿

```
æ—¶é—´çº¿ (4ä¸ªé˜¶æ®µï¼Œ8ä¸ªmicro-batch):

       Micro-batch:  0  1  2  3  4  5  6  7  0  1  2  3  4  5  6  7
       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Stage 0:             F0 F1 F2 F3 F4 F5 F6 F7 B0 B1 B2 B3 B4 B5 B6 B7
Stage 1:                F0 F1 F2 F3 F4 F5 F6 F7 B0 B1 B2 B3 B4 B5 B6 B7
Stage 2:                   F0 F1 F2 F3 F4 F5 F6 F7 B0 B1 B2 B3 B4 B5 B6 B7
Stage 3:                      F0 F1 F2 F3 F4 F5 F6 F7 B0 B1 B2 B3 B4 B5 B6 B7

ç‰¹ç‚¹:
âœ“ åŒæ­¥è®­ç»ƒ
âœ“ æ°”æ³¡ç‡é™ä½: (K-1)/(K-1+M) = 3/11 â‰ˆ 27%
âœ“ ç®€å•å®ç°
âœ— å†…å­˜å³°å€¼é«˜ (éœ€å­˜å‚¨æ‰€æœ‰micro-batchæ¿€æ´»)
```

**æ°”æ³¡ç‡è®¡ç®—**:
```
K = é˜¶æ®µæ•°
M = micro-batchæ•°é‡
æ°”æ³¡ç‡ = (K-1) / (K-1 + M)

ç¤ºä¾‹:
- 4é˜¶æ®µ, 8 micro-batch: æ°”æ³¡ç‡ = 3/11 â‰ˆ 27%
- 4é˜¶æ®µ, 16 micro-batch: æ°”æ³¡ç‡ = 3/19 â‰ˆ 16%
- 8é˜¶æ®µ, 16 micro-batch: æ°”æ³¡ç‡ = 7/23 â‰ˆ 30%
```

### 2. PipeDream - å¼‚æ­¥æµæ°´çº¿

**æ ¸å¿ƒæ€æƒ³**: ä¸åŒmicro-batchä½¿ç”¨ä¸åŒç‰ˆæœ¬çš„æƒé‡

```
æ—¶é—´çº¿:

Stage 0:  F0â”€F1â”€F2â”€F3â”€B0â”€B1â”€B2â”€B3â”€U0â”€F4â”€F5â”€...
Stage 1:  â”€â”€â”€F0â”€F1â”€F2â”€F3â”€B0â”€B1â”€B2â”€B3â”€U0â”€F4â”€...
Stage 2:  â”€â”€â”€â”€â”€â”€F0â”€F1â”€F2â”€F3â”€B0â”€B1â”€B2â”€B3â”€U0â”€...
Stage 3:  â”€â”€â”€â”€â”€â”€â”€â”€â”€F0â”€F1â”€F2â”€F3â”€B0â”€B1â”€B2â”€B3â”€U0

U = Update

ç‰¹ç‚¹:
âœ“ æ°”æ³¡ç‡æ›´ä½
âœ“ å†…å­˜å ç”¨ä½
âœ— ä¸åŒç‰ˆæœ¬æƒé‡(æƒé‡è¿‡æ—¶é—®é¢˜)
âœ— å®ç°å¤æ‚
```

### 3. 1F1B (One Forward One Backward)

**æ ¸å¿ƒæ€æƒ³**: äº¤æ›¿æ‰§è¡Œå‰å‘å’Œåå‘ï¼Œå¹³è¡¡å†…å­˜

```
æ—¶é—´çº¿ (4é˜¶æ®µ, 8 micro-batch):

       å¾®æ‰¹æ¬¡:      0  1  2  3  4  5  6  7  0  1  2  3  4  5  6  7
       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Stage 0:           F0 F1 F2 F3 F4 B0 F5 B1 F6 B2 F7 B3    B4    B5 B6 B7
Stage 1:              F0 F1 F2 F3 B0 F4 B1 F5 B2 F6 B3 F7 B4    B5 B6 B7
Stage 2:                 F0 F1 F2 B0 F3 B1 F4 B2 F5 B3 F6 B4 F7 B5 B6 B7
Stage 3:                    F0 F1 B0 F2 B1 F3 B2 F4 B3 F5 B4 F6 B5 F7 B6 B7

é˜¶æ®µ:
1. Warmup:  å¡«å……æµæ°´çº¿ (çº¯å‰å‘)
2. 1F1B:    äº¤æ›¿å‰å‘/åå‘
3. Cooldown: æ’ç©ºæµæ°´çº¿ (çº¯åå‘)

ç‰¹ç‚¹:
âœ“ å†…å­˜å ç”¨ä½ (åªä¿å­˜Kä¸ªmicro-batchæ¿€æ´»)
âœ“ æ°”æ³¡ç‡ä¸GPipeç›¸åŒ
âœ“ å®ç°ç›¸å¯¹ç®€å•
â†’ å·¥ä¸šç•Œä¸»æµé€‰æ‹©!
```

---

## GPipeè¯¦è§£

### æ¶æ„è®¾è®¡

```python
"""
GPipeæ ¸å¿ƒç»„ä»¶
"""

class GPipe:
    def __init__(self, model, balance, chunks):
        """
        Args:
            model: å®Œæ•´æ¨¡å‹
            balance: æ¯ä¸ªé˜¶æ®µçš„å±‚æ•° [24, 24, 24, 24]
            chunks: micro-batchæ•°é‡
        """
        self.stages = self.partition_model(model, balance)
        self.chunks = chunks
    
    def partition_model(self, model, balance):
        """å°†æ¨¡å‹åˆ†å‰²æˆå¤šä¸ªé˜¶æ®µ"""
        stages = []
        start = 0
        for num_layers in balance:
            end = start + num_layers
            stage = nn.Sequential(*list(model.children())[start:end])
            stages.append(stage)
            start = end
        return stages
```

### å®Œæ•´å®ç°

```python
"""
GPipeå®Œæ•´å®ç°
"""

import torch
import torch.nn as nn
from torch.distributed.pipeline.sync import Pipe


class TransformerBlock(nn.Module):
    """Transformerå—"""
    def __init__(self, dim, heads):
        super().__init__()
        self.attn = nn.MultiheadAttention(dim, heads)
        self.ff = nn.Sequential(
            nn.Linear(dim, 4*dim),
            nn.GELU(),
            nn.Linear(4*dim, dim)
        )
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)
    
    def forward(self, x):
        x = self.norm1(x + self.attn(x, x, x)[0])
        x = self.norm2(x + self.ff(x))
        return x


def create_gpipe_model(num_layers=96, dim=2048, heads=32, devices=4):
    """åˆ›å»ºGPipeæ¨¡å‹"""
    
    # åˆ›å»ºå®Œæ•´æ¨¡å‹
    layers = []
    layers.append(nn.Embedding(50000, dim))
    for _ in range(num_layers):
        layers.append(TransformerBlock(dim, heads))
    layers.append(nn.Linear(dim, 50000))
    
    model = nn.Sequential(*layers)
    
    # è®¡ç®—æ¯ä¸ªè®¾å¤‡çš„å±‚æ•°
    layers_per_device = (num_layers + 2) // devices
    balance = [layers_per_device] * devices
    
    # è°ƒæ•´æœ€åä¸€ä¸ªè®¾å¤‡
    balance[-1] = (num_layers + 2) - sum(balance[:-1])
    
    # åˆ›å»ºPipeæ¨¡å‹
    model = Pipe(
        model,
        balance=balance,
        chunks=8,  # 8ä¸ªmicro-batch
        checkpoint='always'  # æ¿€æ´»æ£€æŸ¥ç‚¹
    )
    
    return model


def train_gpipe():
    """GPipeè®­ç»ƒ"""
    
    # åˆ›å»ºæ¨¡å‹
    model = create_gpipe_model(
        num_layers=96,
        dim=2048, 
        heads=32,
        devices=4
    )
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(10):
        for batch in dataloader:
            # è¾“å…¥æ•°æ®
            input_ids = batch['input_ids']  # [batch_size, seq_len]
            
            # å‰å‘ä¼ æ’­
            # GPipeè‡ªåŠ¨å¤„ç†micro-batchåˆ‡åˆ†
            output = model(input_ids).local_value()
            
            # è®¡ç®—æŸå¤±
            loss = F.cross_entropy(
                output.view(-1, output.size(-1)),
                input_ids.view(-1)
            )
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            print(f"Epoch {epoch}, Loss: {loss.item():.4f}")


# å†…å­˜ä¼˜åŒ–: æ¿€æ´»æ£€æŸ¥ç‚¹
"""
ä¸ä½¿ç”¨æ£€æŸ¥ç‚¹:
- éœ€è¦å­˜å‚¨æ‰€æœ‰micro-batchçš„æ¿€æ´»
- å†…å­˜ = M Ã— activation_size

ä½¿ç”¨æ£€æŸ¥ç‚¹:
- é‡æ–°è®¡ç®—æ¿€æ´»è€Œä¸å­˜å‚¨
- å†…å­˜ = âˆš(M) Ã— activation_size
- æ—¶é—´å¢åŠ  ~33%
"""
```

### GPipeå…³é”®ç‰¹æ€§

#### 1. æ¿€æ´»æ£€æŸ¥ç‚¹

```python
# è‡ªåŠ¨æ¿€æ´»æ£€æŸ¥ç‚¹
pipe_model = Pipe(
    model,
    balance=balance,
    chunks=8,
    checkpoint='always'  # 'always', 'except_last', 'never'
)

"""
æ£€æŸ¥ç‚¹ç­–ç•¥:
- always: æ‰€æœ‰å±‚éƒ½checkpoint (æœ€çœå†…å­˜)
- except_last: æœ€åä¸€ä¸ªstageä¸checkpoint
- never: ä¸ä½¿ç”¨checkpoint (æœ€å¿«)
"""
```

#### 2. Micro-batchè°ƒåº¦

```python
def forward(self, input):
    """GPipeå‰å‘ä¼ æ’­ä¼ªä»£ç """
    
    # åˆ‡åˆ†æˆmicro-batches
    micro_batches = torch.chunk(input, self.chunks, dim=0)
    
    # Warmupé˜¶æ®µ
    outputs = []
    for i in range(self.num_stages):
        for j in range(i + 1):
            micro_batch = micro_batches[j]
            # å‰å‘ä¼ æ’­åˆ°stage i
            output = self.stages[i](micro_batch)
            if i == self.num_stages - 1:
                outputs.append(output)
    
    # ç¨³å®šé˜¶æ®µ
    for i in range(self.num_stages, len(micro_batches)):
        micro_batch = micro_batches[i]
        # å‰å‘
        output = self.forward_stage(micro_batch)
        outputs.append(output)
        
        # åå‘ (ä¸å‰å‘é‡å )
        self.backward_stage(i - self.num_stages)
    
    # Cooldowné˜¶æ®µ
    for i in range(len(micro_batches) - self.num_stages):
        self.backward_stage(len(micro_batches) - self.num_stages + i)
    
    return torch.cat(outputs, dim=0)
```

---

## PipeDreamè¯¦è§£

### æƒé‡ç‰ˆæœ¬ç®¡ç†

```python
"""
PipeDreamçš„æ ¸å¿ƒæŒ‘æˆ˜: ä¸åŒmicro-batchä½¿ç”¨ä¸åŒæƒé‡ç‰ˆæœ¬
"""

class PipeDream:
    def __init__(self, stages, num_versions):
        self.stages = stages
        # æ¯ä¸ªé˜¶æ®µç»´æŠ¤å¤šä¸ªæƒé‡ç‰ˆæœ¬
        self.weight_versions = [
            [copy.deepcopy(stage.state_dict()) 
             for _ in range(num_versions)]
            for stage in stages
        ]
    
    def forward(self, micro_batch, version_id):
        """ä½¿ç”¨ç‰¹å®šç‰ˆæœ¬çš„æƒé‡"""
        # åŠ è½½å¯¹åº”ç‰ˆæœ¬çš„æƒé‡
        self.load_version(version_id)
        
        # å‰å‘ä¼ æ’­
        output = self.stage(micro_batch)
        
        return output, version_id
    
    def backward(self, grad, version_id):
        """ä½¿ç”¨å¯¹åº”ç‰ˆæœ¬çš„æƒé‡è¿›è¡Œåå‘"""
        # åŠ è½½å¯¹åº”ç‰ˆæœ¬
        self.load_version(version_id)
        
        # åå‘ä¼ æ’­
        grad_input = self.stage.backward(grad)
        
        return grad_input
```

### æƒé‡æ›´æ–°ç­–ç•¥

```python
"""
PipeDreamæƒé‡æ›´æ–°
"""

# ç­–ç•¥1: Weight Stashing
def weight_stashing():
    """
    ä¸ºæ¯ä¸ªin-flight micro-batchä¿å­˜ä¸€ä»½æƒé‡
    
    å†…å­˜å¼€é”€: K Ã— weight_size
    K = pipeline depth
    """
    versions = {}
    
    for micro_batch_id in range(num_micro_batches):
        # ä¿å­˜å½“å‰æƒé‡ç‰ˆæœ¬
        version_id = micro_batch_id % pipeline_depth
        versions[version_id] = copy.deepcopy(model.state_dict())
        
        # ä½¿ç”¨å¯¹åº”ç‰ˆæœ¬è¿›è¡Œå‰å‘/åå‘
        output = forward(micro_batch, version=versions[version_id])
        loss.backward()
        
        # æ›´æ–°æƒé‡
        optimizer.step()


# ç­–ç•¥2: Vertical Sync
def vertical_sync():
    """
    å®šæœŸåŒæ­¥æ‰€æœ‰stageçš„æƒé‡
    
    å‡å°‘ç‰ˆæœ¬ä¸ä¸€è‡´
    """
    sync_interval = 4  # æ¯4ä¸ªmicro-batchåŒæ­¥ä¸€æ¬¡
    
    if step % sync_interval == 0:
        # å¹¿æ’­stage 0çš„æƒé‡åˆ°æ‰€æœ‰stage
        for stage_id in range(1, num_stages):
            sync_weights(stage_0, stage_id)
```

---

## 1F1Bè°ƒåº¦

### è¯¦ç»†æ—¶é—´çº¿

```python
"""
1F1Bè°ƒåº¦è¯¦è§£ (4 stages, 8 micro-batches)
"""

def one_f_one_b_schedule():
    num_stages = 4
    num_microbatches = 8
    
    # é˜¶æ®µ1: Warmup (å¡«å……æµæ°´çº¿)
    # æ¯ä¸ªstageä¾æ¬¡å¼€å§‹ï¼Œæ‰§è¡Œ (num_stages - stage_id - 1) æ¬¡å‰å‘
    for stage_id in range(num_stages):
        warmup_iters = num_stages - stage_id - 1
        for i in range(warmup_iters):
            forward(stage_id, microbatch_id=i)
    
    # é˜¶æ®µ2: 1F1B (ç¨³å®šé˜¶æ®µ)
    # äº¤æ›¿æ‰§è¡Œ1æ¬¡å‰å‘å’Œ1æ¬¡åå‘
    num_1f1b_iters = num_microbatches - (num_stages - 1)
    for i in range(num_1f1b_iters):
        for stage_id in range(num_stages):
            # å‰å‘
            forward(stage_id, microbatch_id=warmup_iters + i)
            # åå‘
            backward(stage_id, microbatch_id=i)
    
    # é˜¶æ®µ3: Cooldown (æ’ç©ºæµæ°´çº¿)
    # åªæ‰§è¡Œåå‘ä¼ æ’­
    for stage_id in range(num_stages):
        cooldown_iters = num_stages - stage_id - 1
        for i in range(cooldown_iters):
            backward(stage_id, microbatch_id=num_1f1b_iters + i)
```

### å®Œæ•´å®ç°

```python
"""
1F1Bå®Œæ•´å®ç°
"""

import torch
import torch.nn as nn
import torch.distributed.rpc as rpc


class PipelineStage(nn.Module):
    """æµæ°´çº¿é˜¶æ®µ"""
    
    def __init__(self, stage_id, model, num_stages):
        super().__init__()
        self.stage_id = stage_id
        self.model = model
        self.num_stages = num_stages
        
        # å­˜å‚¨æ¿€æ´»å€¼ (ç”¨äºåå‘ä¼ æ’­)
        self.activations = {}
    
    def forward_stage(self, micro_batch_id, input_tensor):
        """é˜¶æ®µå‰å‘ä¼ æ’­"""
        
        # å‰å‘è®¡ç®—
        with torch.enable_grad():
            output = self.model(input_tensor)
            
            # ä¿å­˜æ¿€æ´»å€¼
            if self.stage_id < self.num_stages - 1:
                self.activations[micro_batch_id] = output.detach()
                output.requires_grad = True
        
        # å‘é€åˆ°ä¸‹ä¸€é˜¶æ®µ
        if self.stage_id < self.num_stages - 1:
            next_stage = self.stage_id + 1
            rpc.rpc_async(
                f'worker{next_stage}',
                forward_stage,
                args=(micro_batch_id, output)
            )
        
        return output
    
    def backward_stage(self, micro_batch_id, grad_output):
        """é˜¶æ®µåå‘ä¼ æ’­"""
        
        # è·å–ä¿å­˜çš„æ¿€æ´»å€¼
        activation = self.activations.pop(micro_batch_id)
        
        # åå‘è®¡ç®—
        activation.backward(grad_output)
        grad_input = activation.grad
        
        # å‘é€åˆ°ä¸Šä¸€é˜¶æ®µ
        if self.stage_id > 0:
            prev_stage = self.stage_id - 1
            rpc.rpc_async(
                f'worker{prev_stage}',
                backward_stage,
                args=(micro_batch_id, grad_input)
            )
        
        return grad_input


def train_1f1b(stage_id, model, num_stages, num_microbatches):
    """1F1Bè®­ç»ƒ"""
    
    stage = PipelineStage(stage_id, model, num_stages)
    optimizer = torch.optim.Adam(stage.parameters())
    
    # Warmupé˜¶æ®µ
    warmup_iters = num_stages - stage_id - 1
    for i in range(warmup_iters):
        if stage_id == 0:
            # ç¬¬ä¸€ä¸ªstageä»dataloaderè·å–æ•°æ®
            input_tensor = next(dataloader)
        else:
            # å…¶ä»–stageæ¥æ”¶ä¸Šä¸€é˜¶æ®µçš„è¾“å‡º
            input_tensor = receive_activation(i)
        
        stage.forward_stage(i, input_tensor)
    
    # 1F1Bé˜¶æ®µ
    num_1f1b_iters = num_microbatches - (num_stages - 1)
    for i in range(num_1f1b_iters):
        # 1 Forward
        micro_batch_id = warmup_iters + i
        if stage_id == 0:
            input_tensor = next(dataloader)
        else:
            input_tensor = receive_activation(micro_batch_id)
        
        stage.forward_stage(micro_batch_id, input_tensor)
        
        # 1 Backward
        if stage_id == num_stages - 1:
            # æœ€åä¸€ä¸ªstageè®¡ç®—æŸå¤±
            grad_output = compute_loss_gradient(i)
        else:
            grad_output = receive_gradient(i)
        
        stage.backward_stage(i, grad_output)
        
        # æ›´æ–°å‚æ•°
        optimizer.step()
        optimizer.zero_grad()
    
    # Cooldowné˜¶æ®µ
    cooldown_iters = num_stages - stage_id - 1
    for i in range(cooldown_iters):
        micro_batch_id = num_1f1b_iters + i
        
        if stage_id == num_stages - 1:
            grad_output = compute_loss_gradient(micro_batch_id)
        else:
            grad_output = receive_gradient(micro_batch_id)
        
        stage.backward_stage(micro_batch_id, grad_output)
        optimizer.step()
        optimizer.zero_grad()
```

### å†…å­˜åˆ†æ

```python
"""
1F1B vs GPipe å†…å­˜å¯¹æ¯”
"""

# GPipeå†…å­˜å ç”¨
gpipe_memory = num_microbatches * activation_size_per_microbatch

# 1F1Bå†…å­˜å ç”¨
one_f_one_b_memory = num_stages * activation_size_per_microbatch

# ç¤ºä¾‹: 4 stages, 16 microbatches
# GPipe:  16 Ã— activation_size
# 1F1B:   4 Ã— activation_size
# èŠ‚çœ:   75% å†…å­˜!
```

---

## å®ç°ç»†èŠ‚

### 1. å±‚åˆ†å‰²ç­–ç•¥

```python
"""
å¦‚ä½•å†³å®šæ¯ä¸ªstageåŒ…å«å¤šå°‘å±‚?
"""

def balance_by_parameters(model, num_stages):
    """æŒ‰å‚æ•°é‡å‡è¡¡åˆ†å‰²"""
    total_params = sum(p.numel() for p in model.parameters())
    params_per_stage = total_params / num_stages
    
    balance = []
    current_params = 0
    current_layers = 0
    
    for layer in model:
        layer_params = sum(p.numel() for p in layer.parameters())
        current_params += layer_params
        current_layers += 1
        
        if current_params >= params_per_stage:
            balance.append(current_layers)
            current_params = 0
            current_layers = 0
    
    return balance


def balance_by_computation(model, num_stages, sample_input):
    """æŒ‰è®¡ç®—é‡å‡è¡¡åˆ†å‰²"""
    from torch.profiler import profile, ProfilerActivity
    
    layer_times = []
    
    # Profileæ¯ä¸€å±‚
    for layer in model:
        with profile(activities=[ProfilerActivity.CUDA]) as prof:
            _ = layer(sample_input)
        
        layer_time = sum([e.cuda_time for e in prof.key_averages()])
        layer_times.append(layer_time)
    
    # è´ªå¿ƒåˆ†å‰²
    total_time = sum(layer_times)
    target_time = total_time / num_stages
    
    balance = []
    current_time = 0
    current_layers = 0
    
    for time in layer_times:
        current_time += time
        current_layers += 1
        
        if current_time >= target_time:
            balance.append(current_layers)
            current_time = 0
            current_layers = 0
    
    return balance
```

### 2. é€šä¿¡ä¼˜åŒ–

```python
"""
ç‚¹å¯¹ç‚¹é€šä¿¡ä¼˜åŒ–
"""

import torch.distributed as dist


class OptimizedPipelineStage:
    def __init__(self, stage_id, prev_rank, next_rank):
        self.stage_id = stage_id
        self.prev_rank = prev_rank
        self.next_rank = next_rank
        
        # é¢„åˆ†é…é€šä¿¡ç¼“å†²åŒº
        self.send_buffer = None
        self.recv_buffer = None
    
    def send_activation(self, tensor):
        """å¼‚æ­¥å‘é€æ¿€æ´»å€¼"""
        if self.send_buffer is None:
            self.send_buffer = tensor.clone()
        else:
            self.send_buffer.copy_(tensor)
        
        # å¼‚æ­¥å‘é€
        handle = dist.isend(self.send_buffer, dst=self.next_rank)
        return handle
    
    def recv_activation(self, shape):
        """å¼‚æ­¥æ¥æ”¶æ¿€æ´»å€¼"""
        if self.recv_buffer is None:
            self.recv_buffer = torch.zeros(shape, device='cuda')
        
        # å¼‚æ­¥æ¥æ”¶
        handle = dist.irecv(self.recv_buffer, src=self.prev_rank)
        return handle, self.recv_buffer
    
    def forward_with_overlap(self, input_tensor):
        """è®¡ç®—å’Œé€šä¿¡é‡å """
        
        # å¼€å§‹æ¥æ”¶ä¸‹ä¸€ä¸ªmicro-batch (å¦‚æœæœ‰)
        if self.has_next_microbatch():
            recv_handle, next_input = self.recv_activation(input_tensor.shape)
        
        # å‰å‘è®¡ç®—å½“å‰micro-batch
        output = self.model(input_tensor)
        
        # å¼‚æ­¥å‘é€è¾“å‡º
        send_handle = self.send_activation(output)
        
        # ç­‰å¾…é€šä¿¡å®Œæˆ
        if self.has_next_microbatch():
            recv_handle.wait()
        send_handle.wait()
        
        return output
```

---

## æ€§èƒ½ä¼˜åŒ–

### 1. Micro-batchæ•°é‡é€‰æ‹©

```python
"""
å¦‚ä½•é€‰æ‹©æœ€ä¼˜çš„micro-batchæ•°é‡?
"""

def optimal_microbatches(num_stages, target_bubble_rate=0.1):
    """
    æ°”æ³¡ç‡ = (num_stages - 1) / (num_stages - 1 + num_microbatches)
    
    æ±‚è§£ num_microbatches:
    num_microbatches = (num_stages - 1) * (1/target_bubble_rate - 1)
    """
    num_microbatches = int((num_stages - 1) * (1/target_bubble_rate - 1))
    return num_microbatches


# ç¤ºä¾‹
for num_stages in [2, 4, 8, 16]:
    optimal_m = optimal_microbatches(num_stages, target_bubble_rate=0.1)
    print(f"Stages: {num_stages}, Optimal Micro-batches: {optimal_m}")

"""
è¾“å‡º:
Stages: 2, Optimal Micro-batches: 9
Stages: 4, Optimal Micro-batches: 27
Stages: 8, Optimal Micro-batches: 63
Stages: 16, Optimal Micro-batches: 135
"""
```

### 2. æ¿€æ´»æ£€æŸ¥ç‚¹

```python
"""
é€‰æ‹©æ€§æ¿€æ´»æ£€æŸ¥ç‚¹
"""

def selective_checkpointing(model, checkpoint_ratio=0.5):
    """
    åªå¯¹éƒ¨åˆ†å±‚ä½¿ç”¨æ£€æŸ¥ç‚¹
    
    ç­–ç•¥: checkpointè®¡ç®—é‡å¤§çš„å±‚
    """
    from torch.utils.checkpoint import checkpoint
    
    layer_costs = profile_layer_costs(model)
    threshold = sorted(layer_costs)[int(len(layer_costs) * checkpoint_ratio)]
    
    class SelectiveCheckpointModel(nn.Module):
        def forward(self, x):
            for i, layer in enumerate(self.layers):
                if layer_costs[i] > threshold:
                    # ä½¿ç”¨checkpoint
                    x = checkpoint(layer, x)
                else:
                    # æ­£å¸¸å‰å‘
                    x = layer(x)
            return x
    
    return SelectiveCheckpointModel()
```

### 3. è™šæ‹Ÿæµæ°´çº¿

```python
"""
è™šæ‹Ÿæµæ°´çº¿ (Interleaved Pipeline)
è¿›ä¸€æ­¥å‡å°‘æ°”æ³¡
"""

def interleaved_schedule(num_stages, num_virtual_stages=2):
    """
    æ¯ä¸ªç‰©ç†stageåŒ…å«å¤šä¸ªè™šæ‹Ÿstage
    
    ä¾‹å¦‚: 4ä¸ªç‰©ç†GPU, æ¯ä¸ªåŒ…å«2ä¸ªè™šæ‹Ÿstage
    æ€»å…±8ä¸ªè™šæ‹Ÿstage
    
    GPU 0: [Stage 0, Stage 4]
    GPU 1: [Stage 1, Stage 5]
    GPU 2: [Stage 2, Stage 6]
    GPU 3: [Stage 3, Stage 7]
    
    æ‰§è¡Œé¡ºåº:
    GPU 0: F0â†’F4â†’F0â†’F4â†’B0â†’B4â†’B0â†’B4
    """
    
    virtual_stages_per_device = num_virtual_stages
    total_virtual_stages = num_stages * virtual_stages_per_device
    
    # äº¤é”™è°ƒåº¦
    schedule = []
    for device_id in range(num_stages):
        device_schedule = []
        for v in range(virtual_stages_per_device):
            stage_id = device_id + v * num_stages
            device_schedule.append(stage_id)
        schedule.append(device_schedule)
    
    return schedule
```

---

## å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1: GPT-3è®­ç»ƒ

```python
"""
ä½¿ç”¨æµæ°´çº¿å¹¶è¡Œè®­ç»ƒGPT-3 (175Bå‚æ•°)
"""

def train_gpt3_pipeline():
    # é…ç½®
    num_layers = 96
    hidden_size = 12288
    num_heads = 96
    num_stages = 8  # 8ä¸ªGPU
    num_microbatches = 64
    
    # åˆ›å»ºæ¨¡å‹
    model = GPT3Model(
        num_layers=num_layers,
        hidden_size=hidden_size,
        num_heads=num_heads
    )
    
    # åˆ†å‰²æ¨¡å‹
    layers_per_stage = num_layers // num_stages  # 12 layers per stage
    balance = [layers_per_stage] * num_stages
    
    # åˆ›å»ºæµæ°´çº¿
    pipe_model = Pipe(
        model,
        balance=balance,
        chunks=num_microbatches,
        checkpoint='always'
    )
    
   # è®­ç»ƒ
    optimizer = torch.optim.AdamW(pipe_model.parameters(), lr=1e-4)
    
    for epoch in range(epochs):
        for batch in dataloader:
            output = pipe_model(batch['input_ids']).local_value()
            loss = F.cross_entropy(output.view(-1, vocab_size), 
                                   batch['labels'].view(-1))
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
```

### æ¡ˆä¾‹2: BERTé¢„è®­ç»ƒ

```python
"""
BERTæµæ°´çº¿å¹¶è¡Œé¢„è®­ç»ƒ
"""

from transformers import BertConfig, BertForPreTraining

def train_bert_pipeline():
    # BERT-Largeé…ç½®
    config = BertConfig(
        vocab_size=30522,
        hidden_size=1024,
        num_hidden_layers=24,
        num_attention_heads=16
    )
    
    model = BertForPreTraining(config)
    
    # 4-stageæµæ°´çº¿
    pipe_model = Pipe(
        model,
        balance=[6, 6, 6, 6],  # æ¯stage 6å±‚
        chunks=16
    )
    
    # MLM + NSPè®­ç»ƒ
    for batch in dataloader:
        input_ids = batch['input_ids']
        token_type_ids = batch['token_type_ids']
        attention_mask = batch['attention_mask']
        labels = batch['labels']
        next_sentence_label = batch['next_sentence_label']
        
        outputs = pipe_model(
            input_ids,
            token_type_ids=token_type_ids,
            attention_mask=attention_mask
        ).local_value()
        
        loss = outputs.loss
        loss.backward()
        optimizer.step()
```

---

## å¸¸è§é—®é¢˜

### Q1: æµæ°´çº¿å¹¶è¡Œ vs æ•°æ®å¹¶è¡Œ?

| ç‰¹æ€§ | æ•°æ®å¹¶è¡Œ | æµæ°´çº¿å¹¶è¡Œ |
|-----|---------|----------|
| **å†…å­˜** | æ¯GPUå®Œæ•´æ¨¡å‹ | æ¯GPUéƒ¨åˆ†æ¨¡å‹ |
| **é€šä¿¡** | AllReduceæ¢¯åº¦ | P2Pæ¿€æ´»/æ¢¯åº¦ |
| **æ•ˆç‡** | é«˜ (>90%) | ä¸­ (70-85%) |
| **é€‚ç”¨** | å°ä¸­å‹æ¨¡å‹ | è¶…æ·±æ¨¡å‹ |

### Q2: å¦‚ä½•é€‰æ‹©é˜¶æ®µæ•°?

```python
"""
ç»éªŒæ³•åˆ™:
- é˜¶æ®µæ•° = GPUæ•°é‡ (ç®€å•)
- è€ƒè™‘é€šä¿¡å¼€é”€: é˜¶æ®µæ•°è¿‡å¤š â†’ é€šä¿¡é¢‘ç¹
- è€ƒè™‘æ°”æ³¡ç‡: é˜¶æ®µæ•°è¿‡å°‘ â†’ åˆ©ç”¨ç‡ä½

æ¨è:
- 2-8ä¸ªé˜¶æ®µ: é€‚åˆå¤§å¤šæ•°åœºæ™¯
- 8-16ä¸ªé˜¶æ®µ: è¶…å¤§æ¨¡å‹
"""
```

### Q3: æµæ°´çº¿å¹¶è¡Œèƒ½ä¸æ•°æ®å¹¶è¡Œç»“åˆå—?

**å¯ä»¥ï¼è¿™å°±æ˜¯2Då¹¶è¡Œ**:

```python
"""
2Då¹¶è¡Œ: æµæ°´çº¿ + æ•°æ®
"""

# 16 GPUsé…ç½®
pipeline_parallel_size = 4  # 4ä¸ªæµæ°´çº¿é˜¶æ®µ
data_parallel_size = 4      # 4è·¯æ•°æ®å¹¶è¡Œ

# GPUåˆ†é…:
# Stage 0: GPU 0,1,2,3    (æ•°æ®å¹¶è¡Œç»„)
# Stage 1: GPU 4,5,6,7
# Stage 2: GPU 8,9,10,11
# Stage 3: GPU 12,13,14,15

# æ¯ä¸ªstageå†…éƒ¨åšæ•°æ®å¹¶è¡Œ
# ä¸åŒstageé—´åšæµæ°´çº¿å¹¶è¡Œ
```

### Q4: æ°”æ³¡æ—¶é—´èƒ½å®Œå…¨æ¶ˆé™¤å—?

**ä¸èƒ½ï¼Œä½†å¯ä»¥æœ€å°åŒ–**:

```
ç†è®ºä¸‹é™ (æ— é™micro-batch):
æ°”æ³¡ç‡ = 0

å®é™…ä¸‹é™:
- GPipe: ~10-20%
- 1F1B: ~10-20%
- è™šæ‹Ÿæµæ°´çº¿: ~5-10%
```

---

## æ€»ç»“

### æµæ°´çº¿å¹¶è¡Œé€‰æ‹©æŒ‡å—

```
å†³ç­–æ ‘:

æ¨¡å‹æ˜¯å¦å¾ˆæ·± (>24å±‚)?
â”œâ”€ No â†’ è€ƒè™‘å…¶ä»–å¹¶è¡Œæ–¹å¼
â””â”€ Yes â†’ ç»§ç»­åˆ¤æ–­

    å†…å­˜æ˜¯å¦å—é™?
    â”œâ”€ Yes â†’ ä½¿ç”¨1F1B (çœå†…å­˜)
    â””â”€ No â†’ ä½¿ç”¨GPipe (ç®€å•)

    æ˜¯å¦è¿½æ±‚æè‡´æ€§èƒ½?
    â”œâ”€ Yes â†’ ä½¿ç”¨è™šæ‹Ÿæµæ°´çº¿
    â””â”€ No â†’ æ ‡å‡†1F1Bè¶³å¤Ÿ
```

### æœ€ä½³å®è·µ

1. âœ… **é¦–é€‰1F1B** - å†…å­˜å’Œæ€§èƒ½å¹³è¡¡æœ€å¥½
2. âœ… **Micro-batchæ•° > 4Ã—é˜¶æ®µæ•°** - æ§åˆ¶æ°”æ³¡ç‡<20%
3. âœ… **æ¿€æ´»æ£€æŸ¥ç‚¹** - å¤§æ¨¡å‹å¿…é¡»å¯ç”¨
4. âœ… **åˆç†åˆ†å‰²** - æŒ‰è®¡ç®—é‡è€Œéå±‚æ•°
5. âœ… **ç»“åˆæ•°æ®å¹¶è¡Œ** - 2Då¹¶è¡Œæ•ˆæœæ›´å¥½

---

## ä¸‹ä¸€æ­¥

å­¦å®Œæµæ°´çº¿å¹¶è¡Œåï¼Œç»§ç»­å­¦ä¹ :
- [å¼ é‡å¹¶è¡Œ](03-tensor-parallelism.md) - å¤„ç†è¶…å®½å±‚
- [æ··åˆå¹¶è¡Œ](06-hybrid-parallelism.md) - ç»„åˆPP+TP+DP

---

<div align="center">
  <strong>æµæ°´çº¿å¹¶è¡Œè®©è¶…æ·±æ¨¡å‹è®­ç»ƒæˆä¸ºå¯èƒ½ï¼ğŸš€</strong>
</div></parameter>