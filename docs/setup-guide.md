# ç¯å¢ƒé…ç½®æŒ‡å—

æœ¬æŒ‡å—å°†å¸®åŠ©ä½ é…ç½®å®Œæ•´çš„åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒï¼ŒåŒ…æ‹¬PyTorchã€LibTorchå’ŒMPIã€‚

## ğŸ“‹ ç›®å½•

- [ç³»ç»Ÿè¦æ±‚](#ç³»ç»Ÿè¦æ±‚)
- [Pythonç¯å¢ƒé…ç½®](#pythonç¯å¢ƒé…ç½®)
- [LibTorché…ç½®](#libtorché…ç½®)
- [MPIé…ç½®](#mpié…ç½®)
- [GPUå’ŒCUDAé…ç½®](#gpuå’Œcudaé…ç½®)
- [ç½‘ç»œé…ç½®](#ç½‘ç»œé…ç½®)
- [éªŒè¯å®‰è£…](#éªŒè¯å®‰è£…)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ç³»ç»Ÿè¦æ±‚

### æœ€ä½é…ç½®
- **æ“ä½œç³»ç»Ÿ**: Linux (Ubuntu 20.04+, CentOS 7+, RHEL 8+)
- **CPU**: 8æ ¸ä»¥ä¸Š
- **å†…å­˜**: 32GB RAM
- **GPU**: NVIDIA GPU (Compute Capability â‰¥ 7.0)
- **å­˜å‚¨**: 100GB å¯ç”¨ç©ºé—´

### æ¨èé…ç½®
- **æ“ä½œç³»ç»Ÿ**: Ubuntu 22.04 LTS
- **CPU**: 32æ ¸ä»¥ä¸Š (AMD EPYC / Intel Xeon)
- **å†…å­˜**: 256GB RAM
- **GPU**: 4Ã— NVIDIA A100 80GB (æˆ– H100)
- **ç½‘ç»œ**: InfiniBand HDR (200Gbps) æˆ– NVLink
- **å­˜å‚¨**: NVMe SSD 1TB+

---

## Pythonç¯å¢ƒé…ç½®

### 1. å®‰è£…Conda

```bash
# ä¸‹è½½Miniconda
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

# å®‰è£…
bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda3

# åˆå§‹åŒ–
source $HOME/miniconda3/bin/activate
conda init bash
source ~/.bashrc
```

### 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
# åˆ›å»ºç¯å¢ƒ
conda create -n dist-parallel python=3.10 -y
conda activate dist-parallel

# å®‰è£…åŸºç¡€å·¥å…·
conda install -y cmake ninja git
```

### 3. å®‰è£…PyTorch

#### CUDA 11.8
```bash
pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 \
    --index-url https://download.pytorch.org/whl/cu118
```

#### CUDA 12.1
```bash
pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 \
    --index-url https://download.pytorch.org/whl/cu121
```

#### CPU Only (æµ‹è¯•ç”¨)
```bash
pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 \
    --index-url https://download.pytorch.org/whl/cpu
```

### 4. å®‰è£…åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶

```bash
# DeepSpeed
pip install deepspeed>=0.10.0

# FairScale
pip install fairscale>=0.4.13

# Accelerate
pip install accelerate>=0.21.0

# å…¶ä»–ä¾èµ–
pip install -r requirements.txt
```

### 5. éªŒè¯PyTorchå®‰è£…

```bash
python -c "import torch; print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')"
python -c "import torch; print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPUæ•°é‡: {torch.cuda.device_count()}')"
python -c "import torch.distributed as dist; print('Distributedæ¨¡å—æ­£å¸¸')"
```

---

## LibTorché…ç½®

### 1. ä¸‹è½½LibTorch

```bash
# é€‰æ‹©ç‰ˆæœ¬
PYTORCH_VERSION="2.1.0"
CUDA_VERSION="cu118"  # æˆ– cu121, cpu

# ä¸‹è½½ (cxx11 ABIç‰ˆæœ¬)
cd /opt
sudo wget https://download.pytorch.org/libtorch/${CUDA_VERSION}/libtorch-cxx11-abi-shared-with-deps-${PYTORCH_VERSION}%2B${CUDA_VERSION}.zip

# è§£å‹
sudo unzip libtorch-cxx11-abi-shared-with-deps-${PYTORCH_VERSION}+${CUDA_VERSION}.zip
sudo mv libtorch /opt/libtorch

# æ¸…ç†
sudo rm libtorch-*.zip
```

### 2. è®¾ç½®ç¯å¢ƒå˜é‡

```bash
# æ·»åŠ åˆ° ~/.bashrc
cat >> ~/.bashrc << 'EOF'
# LibTorch
export LIBTORCH_PATH=/opt/libtorch
export LD_LIBRARY_PATH=$LIBTORCH_PATH/lib:$LD_LIBRARY_PATH
export CMAKE_PREFIX_PATH=$LIBTORCH_PATH:$CMAKE_PREFIX_PATH
EOF

# ä½¿ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ
source ~/.bashrc
```

### 3. éªŒè¯LibTorchå®‰è£…

åˆ›å»ºæµ‹è¯•æ–‡ä»¶ `test_libtorch.cpp`:

```cpp
#include <torch/torch.h>
#include <iostream>

int main() {
    torch::Tensor tensor = torch::rand({2, 3});
    std::cout << "LibTorchå®‰è£…æˆåŠŸï¼" << std::endl;
    std::cout << "Tensor:\n" << tensor << std::endl;
    
    if (torch::cuda::is_available()) {
        std::cout << "CUDAå¯ç”¨ï¼ŒGPUæ•°é‡: " 
                  << torch::cuda::device_count() << std::endl;
    } else {
        std::cout << "CUDAä¸å¯ç”¨" << std::endl;
    }
    
    return 0;
}
```

ç¼–è¯‘å¹¶è¿è¡Œ:

```bash
g++ test_libtorch.cpp -o test_libtorch \
    -I$LIBTORCH_PATH/include \
    -I$LIBTORCH_PATH/include/torch/csrc/api/include \
    -L$LIBTORCH_PATH/lib \
    -ltorch -ltorch_cpu -lc10 \
    -Wl,-rpath,$LIBTORCH_PATH/lib

./test_libtorch
```

---

## MPIé…ç½®

### 1. å®‰è£…MPICH

```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install -y mpich libmpich-dev

# éªŒè¯
mpichversion
which mpirun
```

### 2. å®‰è£…OpenMPI (å¯é€‰)

```bash
# Ubuntu/Debian
sudo apt-get install -y openmpi-bin openmpi-common libopenmpi-dev

# æˆ–è€…ä»æºç ç¼–è¯‘ (æ¨è)
wget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.6.tar.gz
tar -xzf openmpi-4.1.6.tar.gz
cd openmpi-4.1.6

./configure --prefix=/opt/openmpi \
    --enable-mpi-cxx \
    --with-cuda=/usr/local/cuda

make -j$(nproc)
sudo make install

# è®¾ç½®ç¯å¢ƒå˜é‡
export PATH=/opt/openmpi/bin:$PATH
export LD_LIBRARY_PATH=/opt/openmpi/lib:$LD_LIBRARY_PATH
```

### 3. å®‰è£…mpi4py (Pythonç»‘å®š)

```bash
conda activate dist-parallel
pip install mpi4py
```

### 4. éªŒè¯MPIå®‰è£…

```bash
# æ£€æŸ¥ç‰ˆæœ¬
mpirun --version

# æµ‹è¯•MPI
cat > mpi_test.c << 'EOF'
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);
    
    int world_size, world_rank;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
    
    printf("Hello from rank %d of %d\n", world_rank, world_size);
    
    MPI_Finalize();
    return 0;
}
EOF

mpicc mpi_test.c -o mpi_test
mpirun -np 4 ./mpi_test
```

---

## GPUå’ŒCUDAé…ç½®

### 1. æ£€æŸ¥GPU

```bash
# æ£€æŸ¥GPUä¿¡æ¯
nvidia-smi

# æ£€æŸ¥CUDAç‰ˆæœ¬
nvcc --version
cat /usr/local/cuda/version.txt
```

### 2. å®‰è£…CUDA (å¦‚æœæœªå®‰è£…)

```bash
# Ubuntu 22.04 - CUDA 11.8
wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run
sudo sh cuda_11.8.0_520.61.05_linux.run

# è®¾ç½®ç¯å¢ƒå˜é‡
cat >> ~/.bashrc << 'EOF'
export CUDA_HOME=/usr/local/cuda
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
EOF

source ~/.bashrc
```

### 3. å®‰è£…cuDNN

```bash
# ä¸‹è½½cuDNN (éœ€è¦NVIDIAè´¦å·)
# ä» https://developer.nvidia.com/cudnn ä¸‹è½½

# è§£å‹å¹¶å®‰è£…
tar -xvf cudnn-linux-x86_64-8.x.x.x_cudaX.Y-archive.tar.xz
sudo cp cudnn-*-archive/include/cudnn*.h /usr/local/cuda/include
sudo cp cudnn-*-archive/lib/libcudnn* /usr/local/cuda/lib64
sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*
```

### 4. å®‰è£…NCCL

```bash
# Ubuntu
sudo apt-get install -y libnccl2 libnccl-dev

# æˆ–ä»æºç ç¼–è¯‘
git clone https://github.com/NVIDIA/nccl.git
cd nccl
make -j src.build
sudo make install
```

### 5. éªŒè¯CUDAå’ŒNCCL

```bash
# æµ‹è¯•CUDA
python -c "import torch; print(torch.cuda.is_available())"
python -c "import torch; print(torch.cuda.nccl.version())"

# NCCLæµ‹è¯•
git clone https://github.com/NVIDIA/nccl-tests.git
cd nccl-tests
make MPI=1 MPI_HOME=/usr/lib/x86_64-linux-gnu/openmpi
./build/all_reduce_perf -b 8 -e 128M -f 2 -g 4
```

---

## ç½‘ç»œé…ç½®

### 1. å•èŠ‚ç‚¹é…ç½®

```bash
# æ£€æŸ¥ç½‘ç»œæ¥å£
ip addr show

# è®¾ç½®é˜²ç«å¢™ (å¦‚æœéœ€è¦)
sudo ufw allow 29500/tcp  # PyTorché»˜è®¤ç«¯å£
sudo ufw allow 12345/tcp  # è‡ªå®šä¹‰ç«¯å£
```

### 2. å¤šèŠ‚ç‚¹é…ç½®

#### åˆ›å»ºhostfile

```bash
# åˆ›å»º ~/hostfile
cat > ~/hostfile << 'EOF'
node1 slots=4
node2 slots=4
node3 slots=4
node4 slots=4
EOF
```

#### SSHæ— å¯†ç ç™»å½•

```bash
# ç”Ÿæˆå¯†é’¥
ssh-keygen -t rsa -b 4096 -N "" -f ~/.ssh/id_rsa

# å¤åˆ¶åˆ°æ‰€æœ‰èŠ‚ç‚¹
for node in node1 node2 node3 node4; do
    ssh-copy-id $node
done

# æµ‹è¯•
for node in node1 node2 node3 node4; do
    ssh $node "hostname"
done
```

### 3. InfiniBandé…ç½® (å¯é€‰)

```bash
# å®‰è£…é©±åŠ¨
sudo apt-get install -y infiniband-diags ibutils

# æ£€æŸ¥IBçŠ¶æ€
ibstat
ibstatus

# æµ‹è¯•å¸¦å®½
ib_write_bw -d mlx5_0 -a
```

---

## éªŒè¯å®‰è£…

### å®Œæ•´éªŒè¯è„šæœ¬

åˆ›å»º `verify_setup.sh`:

```bash
#!/bin/bash

echo "=== éªŒè¯å®‰è£… ==="
echo

# 1. Pythonå’ŒåŒ…
echo "1. æ£€æŸ¥Pythonç¯å¢ƒ..."
python --version
pip list | grep -E "torch|deepspeed|fairscale|mpi4py"
echo

# 2. PyTorch
echo "2. æ£€æŸ¥PyTorch..."
python << EOF
import torch
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
print(f"NCCLç‰ˆæœ¬: {torch.cuda.nccl.version()}")
EOF
echo

# 3. MPI
echo "3. æ£€æŸ¥MPI..."
mpirun --version
which mpirun
echo

# 4. CUDA
echo "4. æ£€æŸ¥CUDA..."
nvcc --version
nvidia-smi --query-gpu=name,memory.total --format=csv
echo

# 5. ç½‘ç»œ
echo "5. æ£€æŸ¥ç½‘ç»œ..."
ip addr show | grep inet
echo

# 6. ç¯å¢ƒå˜é‡
echo "6. æ£€æŸ¥ç¯å¢ƒå˜é‡..."
echo "CUDA_HOME: $CUDA_HOME"
echo "LIBTORCH_PATH: $LIBTORCH_PATH"
echo

echo "=== éªŒè¯å®Œæˆ ==="
```

è¿è¡ŒéªŒè¯:

```bash
chmod +x verify_setup.sh
./verify_setup.sh
```

### å¿«é€Ÿæµ‹è¯•è„šæœ¬

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/your-username/MatrixDistributedComputing-5DParallel.git
cd MatrixDistributedComputing-5DParallel

# æµ‹è¯•æ•°æ®å¹¶è¡Œ
cd 01-data-parallelism/pytorch
torchrun --nproc_per_node=2 dp_basic.py

# æµ‹è¯•MPI
cd ../../01-data-parallelism/cpp
make
mpirun -np 2 ./dp_mpi
```

---

## å¸¸è§é—®é¢˜

### Q1: NCCLåˆå§‹åŒ–å¤±è´¥

**é—®é¢˜**: `NCCL error: unhandled system error`

**è§£å†³**:
```bash
# æ£€æŸ¥NCCLç‰ˆæœ¬å…¼å®¹æ€§
python -c "import torch; print(torch.cuda.nccl.version())"

# è®¾ç½®ç¯å¢ƒå˜é‡
export NCCL_DEBUG=INFO
export NCCL_IB_DISABLE=1  # ç¦ç”¨InfiniBand (æµ‹è¯•ç”¨)
```

### Q2: LibTorché“¾æ¥é”™è¯¯

**é—®é¢˜**: `cannot find -ltorch`

**è§£å†³**:
```bash
# ç¡®è®¤è·¯å¾„
echo $LIBTORCH_PATH
ls -l $LIBTORCH_PATH/lib/libtorch.so

# é‡æ–°è®¾ç½®LD_LIBRARY_PATH
export LD_LIBRARY_PATH=$LIBTORCH_PATH/lib:$LD_LIBRARY_PATH
ldconfig
```

### Q3: MPIè¿›ç¨‹é€šä¿¡è¶…æ—¶

**é—®é¢˜**: `MPI_Init timeout`

**è§£å†³**:
```bash
# æ£€æŸ¥SSHè¿æ¥
ssh localhost hostname

# æ£€æŸ¥é˜²ç«å¢™
sudo ufw status
sudo ufw allow from 192.168.0.0/16

# ä½¿ç”¨localhostç¯å›
mpirun -np 4 --host localhost:4 ./your_program
```

### Q4: GPUå†…å­˜ä¸è¶³

**é—®é¢˜**: `CUDA out of memory`

**è§£å†³**:
```python
# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
model.gradient_checkpointing_enable()

# ä½¿ç”¨æ··åˆç²¾åº¦
from torch.cuda.amp import autocast
with autocast():
    output = model(input)

# å‡å°batch size
batch_size = batch_size // 2
```

### Q5: å¤šèŠ‚ç‚¹é€šä¿¡å¤±è´¥

**é—®é¢˜**: `Connection refused`

**è§£å†³**:
```bash
# è®¾ç½®æ­£ç¡®çš„masteråœ°å€
export MASTER_ADDR=<master_node_ip>
export MASTER_PORT=29500

# æ£€æŸ¥ç«¯å£æ˜¯å¦å¼€æ”¾
nc -zv $MASTER_ADDR $MASTER_PORT

# ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½èƒ½è®¿é—®
ping -c 3 $MASTER_ADDR
```

---

## Dockeréƒ¨ç½² (å¯é€‰)

### ä½¿ç”¨Dockerå¿«é€Ÿå¯åŠ¨

```bash
# æ„å»ºé•œåƒ
docker build -t dist-parallel:latest -f docker/Dockerfile.pytorch .

# è¿è¡Œå®¹å™¨
docker run --gpus all --ipc=host --ulimit memlock=-1 \
    -v $(pwd):/workspace \
    -it dist-parallel:latest bash

# æµ‹è¯•
torchrun --nproc_per_node=2 01-data-parallelism/pytorch/dp_basic.py
```

---

## æ€§èƒ½è°ƒä¼˜å»ºè®®

### ç³»ç»Ÿçº§ä¼˜åŒ–

```bash
# 1. è®¾ç½®CPUäº²å’Œæ€§
export OMP_NUM_THREADS=8
export MKL_NUM_THREADS=8

# 2. ç¦ç”¨CPUé™é¢‘
sudo cpupower frequency-set -g performance

# 3. å¢å¤§å…±äº«å†…å­˜
sudo sysctl -w kernel.shmmax=68719476736
sudo sysctl -w kernel.shmall=16777216

# 4. ä¼˜åŒ–ç½‘ç»œç¼“å†²åŒº
sudo sysctl -w net.core.rmem_max=134217728
sudo sysctl -w net.core.wmem_max=134217728
```

### PyTorchä¼˜åŒ–

```python
# å¯ç”¨cudnn benchmark
torch.backends.cudnn.benchmark = True

# å¯ç”¨TF32 (Ampereæ¶æ„)
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# è®¾ç½®NCCLä¼˜åŒ–
os.environ['NCCL_IB_GID_INDEX'] = '3'
os.environ['NCCL_SOCKET_IFNAME'] = 'eth0'
```

---

## ä¸‹ä¸€æ­¥

ç¯å¢ƒé…ç½®å®Œæˆåï¼Œä½ å¯ä»¥ï¼š

1. ğŸ“– é˜…è¯» [00-introduction.md](00-introduction.md) äº†è§£åŸºç¡€æ¦‚å¿µ
2. ğŸ’» è¿è¡Œ [01-data-parallelism](../01-data-parallelism) ä¸­çš„ç¤ºä¾‹
3. ğŸ“Š æŸ¥çœ‹ [benchmarks.md](benchmarks.md) äº†è§£æ€§èƒ½æŒ‡æ ‡
4. ğŸš€ å¼€å§‹è®­ç»ƒä½ çš„ç¬¬ä¸€ä¸ªåˆ†å¸ƒå¼æ¨¡å‹ï¼

---

<div align="center">
  <strong>Happy Coding! ğŸ‰</strong>
</div>