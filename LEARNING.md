# 5Då¹¶è¡Œå­¦ä¹ èµ„æºä¸å‚è€ƒèµ„æ–™

è¿™æ˜¯ä¸€ä»½è¯¦å°½çš„å­¦ä¹ èµ„æºæ¸…å•ï¼Œæ¶µç›–ç†è®ºã€è®ºæ–‡ã€å®˜æ–¹æ–‡æ¡£ã€æ•™ç¨‹å’Œå¼€æºé¡¹ç›®ã€‚

---

## ç›®å½•
1. [åŸºç¡€çŸ¥è¯†](#1-åŸºç¡€çŸ¥è¯†)
2. [æ•°æ®å¹¶è¡Œ (DP)](#2-æ•°æ®å¹¶è¡Œ-dp)
3. [æµæ°´çº¿å¹¶è¡Œ (PP)](#3-æµæ°´çº¿å¹¶è¡Œ-pp)
4. [å¼ é‡å¹¶è¡Œ (TP)](#4-å¼ é‡å¹¶è¡Œ-tp)
5. [åºåˆ—å¹¶è¡Œ (SP)](#5-åºåˆ—å¹¶è¡Œ-sp)
6. [ä¸“å®¶å¹¶è¡Œ (EP)](#6-ä¸“å®¶å¹¶è¡Œ-ep)
7. [æ··åˆå¹¶è¡Œ](#7-æ··åˆå¹¶è¡Œ)
8. [å·¥ç¨‹å®è·µ](#8-å·¥ç¨‹å®è·µ)
9. [å¼€æºæ¡†æ¶](#9-å¼€æºæ¡†æ¶)
10. [è§†é¢‘æ•™ç¨‹](#10-è§†é¢‘æ•™ç¨‹)

---

## 1. åŸºç¡€çŸ¥è¯†

### 1.1 åˆ†å¸ƒå¼è®¡ç®—åŸºç¡€

#### ğŸ“š ä¹¦ç±
- **ã€ŠDistributed Computing: Principles, Algorithms, and Systemsã€‹** by Ajay D. Kshemkalyani
  - åˆ†å¸ƒå¼ç³»ç»Ÿç»å…¸æ•™æ
  - æ¶µç›–åŸºæœ¬åŸç†å’Œç®—æ³•

- **ã€ŠParallel Programming in C with MPI and OpenMPã€‹** by Michael J. Quinn
  - MPIå’ŒOpenMPç¼–ç¨‹æŒ‡å—
  - é€‚åˆC++å®ç°å‚è€ƒ

#### ğŸ“„ è®ºæ–‡
- **"Data Parallelism"** - Ian Foster (1995)
  - æ•°æ®å¹¶è¡Œçš„å¼€åˆ›æ€§è®ºæ–‡
  - ğŸ“ [PDF](https://www.mcs.anl.gov/~itf/dbpp/)

- **"Efficient Large-Scale Language Model Training"** - Shoeybi et al. (2019)
  - å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒç»¼è¿°
  - ğŸ“ [arXiv:1909.08053](https://arxiv.org/abs/1909.08053)

#### ğŸŒ åœ¨çº¿èµ„æº
- **PyTorch Distributed Overview**
  - https://pytorch.org/tutorials/beginner/dist_overview.html
  - PyTorchå®˜æ–¹åˆ†å¸ƒå¼æ•™ç¨‹

- **NCCL Documentation**
  - https://docs.nvidia.com/deeplearning/nccl/
  - GPUé—´é€šä¿¡åº“æ–‡æ¡£

- **MPI Tutorial**
  - https://mpitutorial.com/
  - è¯¦ç»†çš„MPIç¼–ç¨‹æ•™ç¨‹

### 1.2 é€šä¿¡åŸè¯­

#### ğŸ“º è§†é¢‘
- **"Understanding Collective Communication"** - NVIDIA
  - https://www.youtube.com/watch?v=KJGlMRPe-bw
  - AllReduceã€AllGatherç­‰åŸè¯­è®²è§£

#### ğŸ“– æ–‡æ¡£
- **NCCL Collective Operations**
  - https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html
  - è¯¦ç»†çš„é›†åˆé€šä¿¡æ“ä½œè¯´æ˜

---

## 2. æ•°æ®å¹¶è¡Œ (DP)

### 2.1 æ ¸å¿ƒè®ºæ–‡

1. **"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"**
   - Goyal et al., Facebook AI Research (2017)
   - ğŸ“ [arXiv:1706.02677](https://arxiv.org/abs/1706.02677)
   - ğŸ’¡ å…³é”®å†…å®¹ï¼šå¤§æ‰¹é‡è®­ç»ƒã€å­¦ä¹ ç‡ç¼©æ”¾ã€warm-upç­–ç•¥

2. **"PyTorch Distributed: Experiences on Accelerating Data Parallel Training"**
   - Li et al. (2020)
   - ğŸ“ [arXiv:2006.15704](https://arxiv.org/abs/2006.15704)
   - ğŸ’¡ å…³é”®å†…å®¹ï¼šDDPå®ç°ç»†èŠ‚ã€ä¼˜åŒ–æŠ€å·§

3. **"ZeRO: Memory Optimizations Toward Training Trillion Parameter Models"**
   - Rajbhandari et al., Microsoft (2020)
   - ğŸ“ [arXiv:1910.02054](https://arxiv.org/abs/1910.02054)
   - ğŸ’¡ å…³é”®å†…å®¹ï¼šå‚æ•°åˆ†ç‰‡ã€æ¢¯åº¦åˆ†ç‰‡ã€ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡

### 2.2 å®˜æ–¹æ–‡æ¡£

#### PyTorch
- **DistributedDataParallel (DDP)**
  - https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html
  - DDP APIå®Œæ•´æ–‡æ¡£

- **DDP Tutorial**
  - https://pytorch.org/tutorials/intermediate/ddp_tutorial.html
  - å®Œæ•´çš„DDPä½¿ç”¨æ•™ç¨‹ï¼ŒåŒ…å«ä»£ç ç¤ºä¾‹

- **FSDP Tutorial**
  - https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html
  - Fully Sharded Data Parallelæ•™ç¨‹

#### TensorFlow
- **Distributed Training Guide**
  - https://www.tensorflow.org/guide/distributed_training
  - TensorFlowåˆ†å¸ƒå¼è®­ç»ƒæŒ‡å—

### 2.3 å¼€æºå®ç°

- **Horovod**
  - https://github.com/horovod/horovod
  - Uberå¼€å‘çš„åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶
  - æ”¯æŒTensorFlowã€PyTorchã€MXNet

- **Hivemind**
  - https://github.com/learning-at-home/hivemind
  - å»ä¸­å¿ƒåŒ–æ·±åº¦å­¦ä¹ 

### 2.4 åšå®¢æ–‡ç« 

- **"Introduction to Distributed Data Parallel"** - PyTorch Blog
  - https://pytorch.org/tutorials/intermediate/ddp_tutorial.html
  
- **"Efficient Training on Multiple GPUs"** - Hugging Face
  - https://huggingface.co/docs/transformers/perf_train_gpu_many

---

## 3. æµæ°´çº¿å¹¶è¡Œ (PP)

### 3.1 æ ¸å¿ƒè®ºæ–‡

1. **"GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism"**
   - Huang et al., Google (2019)
   - ğŸ“ [arXiv:1811.06965](https://arxiv.org/abs/1811.06965)
   - ğŸ’¡ å…³é”®å†…å®¹ï¼šå¾®æ‰¹æ¬¡æµæ°´çº¿ã€åŒæ­¥è®­ç»ƒã€å†…å­˜ä¼˜åŒ–

2. **"PipeDream: Generalized Pipeline Parallelism for DNN Training"**
   - Narayanan et al., Microsoft (2019)
   - ğŸ“ [SOSP 2019](https://cs.stanford.edu/~matei/papers/2019/sosp_pipedream.pdf)
   - ğŸ’¡ å…³é”®å†…å®¹ï¼šå¼‚æ­¥æµæ°´çº¿ã€æƒé‡ç‰ˆæœ¬ç®¡ç†ã€1F1Bè°ƒåº¦

3. **"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"**
   - Shoeybi et al., NVIDIA (2020)
   - ğŸ“ [arXiv:1909.08053](https://arxiv.org/abs/1909.08053)
   - ğŸ’¡ å…³é”®å†…å®¹ï¼šç»“åˆTPå’ŒPP

4. **"Chimera: Efficiently Training Large-Scale Neural Networks"**
   - Li et al. (2021)
   - ğŸ“ [SOSP 2021](https://dl.acm.org/doi/10.1145/3477132.3483547)
   - ğŸ’¡ å…³é”®å†…å®¹ï¼šåŠ¨æ€æµæ°´çº¿è°ƒåº¦

### 3.2 å®˜æ–¹æ–‡æ¡£

#### PyTorch
- **torch.distributed.pipeline**
  - https://pytorch.org/docs/stable/pipeline.html
  - PyTorchå®˜æ–¹æµæ°´çº¿å¹¶è¡ŒAPI

- **Pipeline Parallelism Tutorial**
  - https://pytorch.org/tutorials/intermediate/dist_pipeline_parallel_tutorial.html

#### DeepSpeed
- **Pipeline Parallelism**
  - https://www.deepspeed.ai/tutorials/pipeline/
  - DeepSpeedçš„æµæ°´çº¿å¹¶è¡Œæ•™ç¨‹

### 3.3 å¼€æºå®ç°

- **FairScale Pipeline**
  - https://github.com/facebookresearch/fairscale
  - Facebookçš„æ¨¡å‹å¹¶è¡Œåº“

- **PipeDream**
  - https://github.com/msr-fiddle/pipedream
  - PipeDreamå®˜æ–¹å®ç°

### 3.4 æŠ€æœ¯åšå®¢

- **"How to Train Really Large Models on Many GPUs?"** - Lilian Weng
  - https://lilianweng.github.io/posts/2021-09-25-train-large/
  - å¤§æ¨¡å‹è®­ç»ƒå…¨é¢ç»¼è¿°

---

## 4. å¼ é‡å¹¶è¡Œ (TP)

### 4.1 æ ¸å¿ƒè®ºæ–‡

1. **"Megatron-LM: Training Multi-Billion Parameter Language Models"**
   - Shoeybi et al., NVIDIA (2019)
   - ğŸ“ [arXiv:1909.08053](https://arxiv.org/abs/1909.08053)
   - ğŸ’¡ å…³é”®å†…å®¹ï¼šåˆ—å¹¶è¡Œã€è¡Œå¹¶è¡Œã€é€šä¿¡ä¼˜åŒ–

2. **"Tensor Parallelism in Large-Scale Transformers"**
   - ğŸ“ Megatron-LM GitHub: https://github.com/NVIDIA/Megatron-LM
   - ğŸ’¡ å…³é”®å†…å®¹ï¼šTransformerå±‚çš„å¼ é‡åˆ‡åˆ†ç­–ç•¥

3. **"Colossal-AI: A Unified Deep Learning System"**
   - Li et al. (2021)
   - ğŸ“ [arXiv:2110.14883](https://arxiv.org/abs/2110.14883)
   - ğŸ’¡ å…³é”®å†…å®¹ï¼šå¤šç»´å¼ é‡å¹¶è¡Œ

### 4.2 å®˜æ–¹æ–‡æ¡£

#### Megatron-LM
- **Megatron-LM Documentation**
  - https://github.com/NVIDIA/Megatron-LM
  - NVIDIAå®˜æ–¹å®ç°å’Œæ–‡æ¡£

- **Tensor and Pipeline Parallelism**
  - https://github.com/NVIDIA/Megatron-LM/blob/main/docs/PARALLELISM.md

#### DeepSpeed
- **Model Parallelism**
  - https://www.deepspeed.ai/training/#model-parallelism
  - DeepSpeedçš„æ¨¡å‹å¹¶è¡ŒæŒ‡å—

### 4.3 å¼€æºå®ç°

- **Megatron-LM**
  - https://github.com/NVIDIA/Megatron-LM
  - NVIDIAå®˜æ–¹å®ç°ï¼Œæœ€æƒå¨

- **Colossal-AI**
  - https://github.com/hpcaitech/ColossalAI
  - æ”¯æŒå¤šç§å¹¶è¡Œç­–ç•¥

- **Alpa**
  - https://github.com/alpa-projects/alpa
  - è‡ªåŠ¨å¹¶è¡Œä¼˜åŒ–

### 4.4 æ•™ç¨‹ä¸åšå®¢

- **"Tensor Parallelism in PyTorch"** - Lei Mao's Blog
  - https://leimao.github.io/blog/PyTorch-Distributed-Training/

- **"Understanding Tensor Parallelism"** - Hugging Face
  - https://huggingface.co/docs/transformers/v4.15.0/parallelism

---

## 5. åºåˆ—å¹¶è¡Œ (SP)

### 5.1 æ ¸å¿ƒè®ºæ–‡

1. **"Reducing Activation Recomputation in Large Transformer Models"**
   - Korthikanti et al., NVIDIA (2022)
   - ğŸ“ [arXiv:2205.05198](https://arxiv.org/abs/2205.05198)
   - ğŸ’¡ å…³é”®å†…å®¹ï¼šåºåˆ—ç»´åº¦åˆ†å‰²ã€æ¿€æ´»å†…å­˜ä¼˜åŒ–

2. **"Sequence Parallelism: Long Sequence Training from System Perspective"**
   - Li et al. (2021)
   - ğŸ“ [arXiv:2105.13120](https://arxiv.org/abs/2105.13120)
   - ğŸ’¡ å…³é”®å†…å®¹ï¼šRing Attentionã€å—çŠ¶åºåˆ—å¤„ç†

3. **"DeepSpeed Ulysses: System Optimizations for Enabling Training"**
   - Jacobs et al., Microsoft (2023)
   - ğŸ“ [DeepSpeed Blog](https://www.microsoft.com/en-us/research/blog/deepspeed-ulysses/)
   - ğŸ’¡ å…³é”®å†…å®¹ï¼šAll-to-Allé€šä¿¡ä¼˜åŒ–

### 5.2 å®˜æ–¹æ–‡æ¡£

#### DeepSpeed
- **Sequence Parallelism**
  - https://www.deepspeed.ai/tutorials/ds-sequence/
  - DeepSpeedåºåˆ—å¹¶è¡Œæ•™ç¨‹

- **DeepSpeed Ulysses**
  - https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ulysses

#### Megatron-LM
- **Sequence Parallel in Megatron**
  - https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/tensor_parallel/

### 5.3 å¼€æºå®ç°

- **Ring Attention**
  - https://github.com/lhao499/ring-attention
  - è¶…é•¿åºåˆ—æ³¨æ„åŠ›å®ç°

- **FlashAttention**
  - https://github.com/Dao-AILab/flash-attention
  - é«˜æ•ˆæ³¨æ„åŠ›å®ç°ï¼Œå¯ä¸SPç»“åˆ

### 5.4 æŠ€æœ¯æ–‡ç« 

- **"Long Sequence Training from System Perspective"**
  - https://www.microsoft.com/en-us/research/blog/deepspeed-ulysses-system-optimizations-for-enabling-training-of-extreme-long-sequence-transformer-models/

---

## 6. ä¸“å®¶å¹¶è¡Œ (EP)

### 6.1 æ ¸å¿ƒè®ºæ–‡

1. **"Switch Transformers: Scaling to Trillion Parameter Models"**
   - Fedus et al., Google (2021)
   - ğŸ“ [arXiv:2101.03961](https://arxiv.org/abs/2101.03961)
   - ğŸ’¡ å…³é”®å†…å®¹ï¼šç®€åŒ–çš„MoEè·¯ç”±ã€ä¸“å®¶å®¹é‡

2. **"GShard: Scaling Giant Models with Conditional Computation"**
   - Lepikhin et al., Google (2020)
   - ğŸ“ [arXiv:2006.16668](https://arxiv.org/abs/2006.16668)
   - ğŸ’¡ å…³é”®å†…å®¹ï¼šåˆ†ç‰‡MoEã€è´Ÿè½½å‡è¡¡

3. **"BASE Layers: Simplifying Training of Large Models"**
   - Lewis et al., Meta (2021)
   - ğŸ“ [arXiv:2103.16716](https://arxiv.org/abs/2103.16716)
   - ğŸ’¡ å…³é”®å†…å®¹ï¼šä¸“å®¶è®­ç»ƒç¨³å®šæ€§

4. **"ST-MoE: Designing Stable and Transferable MoE Models"**
   - Zoph et al., Google (2022)
   - ğŸ“ [arXiv:2202.08906](https://arxiv.org/abs/2202.08906)
   - ğŸ’¡ å…³é”®å†…å®¹ï¼šè·¯ç”±å™¨è®¾è®¡ã€ä¸“å®¶åˆå§‹åŒ–

### 6.2 å®˜æ–¹æ–‡æ¡£

#### DeepSpeed
- **MoE Training**
  - https://www.deepspeed.ai/tutorials/mixture-of-experts/
  - DeepSpeed MoEå®Œæ•´æ•™ç¨‹

- **DeepSpeed-MoE API**
  - https://deepspeed.readthedocs.io/en/latest/moe.html

#### FairSeq
- **MoE Implementation**
  - https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm
  - Metaçš„MoEå®ç°

### 6.3 å¼€æºå®ç°

- **DeepSpeed-MoE**
  - https://github.com/microsoft/DeepSpeedExamples/tree/master/MoE
  - Microsoftå®˜æ–¹MoEç¤ºä¾‹

- **Tutel**
  - https://github.com/microsoft/tutel
  - é«˜æ€§èƒ½MoEåº“

- **Switch Transformers**
  - https://github.com/google-research/t5x/tree/main/t5x/examples/scalable_t5
  - Googleå®˜æ–¹å®ç°

- **Mixtral**
  - https://github.com/mistralai/mistral-src
  - Mistral AIçš„å¼€æºMoEæ¨¡å‹

### 6.4 æŠ€æœ¯åšå®¢

- **"Mixture of Experts Explained"** - Hugging Face
  - https://huggingface.co/blog/moe

- **"Scaling to MoE Models"** - Microsoft Research
  - https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/

---

## 7. æ··åˆå¹¶è¡Œ

### 7.1 æ ¸å¿ƒè®ºæ–‡

1. **"Efficient Large-Scale Language Model Training on GPU Clusters"**
   - Narayanan et al., NVIDIA (2021)
   - ğŸ“ [arXiv:2104.04473](https://arxiv.org/abs/2104.04473)
   - ğŸ’¡ å…³é”®å†…å®¹ï¼š3Då¹¶è¡Œ (DP+PP+TP)

2. **"Colossal-AI: A Unified Deep Learning System"**
   - Li et al., HPC-AI Tech (2021)
   - ğŸ“ [arXiv:2110.14883](https://arxiv.org/abs/2110.14883)
   - ğŸ’¡ å…³é”®å†…å®¹ï¼šå¤šç»´å¹¶è¡Œè‡ªåŠ¨åŒ–

3. **"Alpa: Automating Inter- and Intra-Operator Parallelism"**
   - Zheng et al., UC Berkeley (2022)
   - ğŸ“ [OSDI 2022](https://arxiv.org/abs/2201.12023)
   - ğŸ’¡ å…³é”®å†…å®¹ï¼šè‡ªåŠ¨å¹¶è¡Œç­–ç•¥æœç´¢

### 7.2 å®˜æ–¹æ–‡æ¡£

#### DeepSpeed
- **3D Parallelism**
  - https://www.deepspeed.ai/tutorials/megatron/
  - DeepSpeed + Megatroné›†æˆ

#### Megatron-LM
- **Multi-Dimensional Parallelism**
  - https://github.com/NVIDIA/Megatron-LM/blob/main/examples/pretrain_gpt_distributed.sh
  - å®Œæ•´çš„å¤šç»´å¹¶è¡Œå¯åŠ¨è„šæœ¬

### 7.3 å¼€æºæ¡†æ¶

- **Megatron-DeepSpeed**
  - https://github.com/microsoft/Megatron-DeepSpeed
  - ç»“åˆä¸¤å¤§æ¡†æ¶çš„ä¼˜åŠ¿

- **Colossal-AI**
  - https://github.com/hpcaitech/ColossalAI
  - æ”¯æŒå„ç§å¹¶è¡Œç»„åˆ

---

## 8. å·¥ç¨‹å®è·µ

### 8.1 æ€§èƒ½ä¼˜åŒ–

#### è®ºæ–‡
- **"ZeRO-Infinity: Breaking GPU Memory Wall"**
  - Rajbhandari et al. (2021)
  - ğŸ“ [arXiv:2104.07857](https://arxiv.org/abs/2104.07857)

- **"Activation Checkpointing"**
  - Chen et al. (2016)
  - ğŸ“ [arXiv:1604.06174](https://arxiv.org/abs/1604.06174)

#### æ–‡æ¡£
- **PyTorch Performance Tuning**
  - https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html

- **NVIDIA NCCL Best Practices**
  - https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/best_practices.html

### 8.2 è°ƒè¯•ä¸ç›‘æ§

- **TensorBoard Profiling**
  - https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras

- **PyTorch Profiler**
  - https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html

- **NVIDIA Nsight Systems**
  - https://developer.nvidia.com/nsight-systems

### 8.3 é…ç½®ç¤ºä¾‹

- **DeepSpeed Configuration JSON**
  - https://www.deepspeed.ai/docs/config-json/

- **Megatron Launch Scripts**
  - https://github.com/NVIDIA/Megatron-LM/tree/main/examples

---

## 9. å¼€æºæ¡†æ¶

### 9.1 ä¸»æµæ¡†æ¶å¯¹æ¯”

| æ¡†æ¶ | DP | PP | TP | SP | EP | æ˜“ç”¨æ€§ | æ€§èƒ½ |
|------|----|----|----|----|----|----|------|
| **DeepSpeed** | âœ… | âœ… | âœ… | âœ… | âœ… | â­â­â­â­ | â­â­â­â­â­ |
| **Megatron-LM** | âœ… | âœ… | âœ… | âœ… | âŒ | â­â­â­ | â­â­â­â­â­ |
| **Colossal-AI** | âœ… | âœ… | âœ… | âœ… | âœ… | â­â­â­â­ | â­â­â­â­ |
| **FairScale** | âœ… | âœ… | âŒ | âŒ | âŒ | â­â­â­â­â­ | â­â­â­ |
| **Alpa** | âœ… | âœ… | âœ… | âŒ | âŒ | â­â­â­â­â­ | â­â­â­â­ |

### 9.2 æ¡†æ¶è¯¦ç»†ä»‹ç»

#### DeepSpeed
- **GitHub**: https://github.com/microsoft/DeepSpeed
- **æ–‡æ¡£**: https://www.deepspeed.ai/
- **ç‰¹ç‚¹**: Microsoftå¼€å‘ï¼ŒåŠŸèƒ½æœ€å…¨ï¼Œç¤¾åŒºæ´»è·ƒ
- **æ¨èåœºæ™¯**: å·¥ä¸šçº§å¤§æ¨¡å‹è®­ç»ƒ

#### Megatron-LM
- **GitHub**: https://github.com/NVIDIA/Megatron-LM
- **ç‰¹ç‚¹**: NVIDIAå¼€å‘ï¼Œæ€§èƒ½æœ€ä¼˜
- **æ¨èåœºæ™¯**: GPUé›†ç¾¤ä¸Šçš„è¶…å¤§æ¨¡å‹

#### Colossal-AI
- **GitHub**: https://github.com/hpcaitech/ColossalAI
- **æ–‡æ¡£**: https://colossalai.org/
- **ç‰¹ç‚¹**: æ˜“ç”¨æ€§å¥½ï¼Œè‡ªåŠ¨åŒ–ç¨‹åº¦é«˜
- **æ¨èåœºæ™¯**: å¿«é€ŸåŸå‹å¼€å‘

---

## 10. è§†é¢‘æ•™ç¨‹

### 10.1 å…¥é—¨è¯¾ç¨‹

- **"Distributed Deep Learning"** - Stanford CS336
  - https://stanford-cs336.github.io/spring2024/
  - æ–¯å¦ç¦å¤§å­¦è¯¾ç¨‹

- **"Large Language Models"** - UW CSE 599
  - https://courses.cs.washington.edu/courses/cse599g1/
  - åç››é¡¿å¤§å­¦è¯¾ç¨‹

### 10.2 æŠ€æœ¯è®²åº§

- **"Training GPT-3 Scale Models"** - NVIDIA GTC
  - YouTube: NVIDIA Developer Channel
  - Megatron-LMæŠ€æœ¯è¯¦è§£

- **"DeepSpeed: Extreme-scale Model Training"** - Microsoft
  - https://www.youtube.com/watch?v=wbG0jGU5qvY

### 10.3 ä¼šè®®æ¼”è®²

- **MLSys Conference**
  - https://mlsys.org/
  - ç³»ç»Ÿä¸æœºå™¨å­¦ä¹ ä¼šè®®å½•åƒ

- **NVIDIA GTC Sessions**
  - https://www.nvidia.com/gtc/
  - GPUæŠ€æœ¯å¤§ä¼š

---

## 11. å®æˆ˜é¡¹ç›®

### 11.1 æ¨¡å‹è®­ç»ƒç¤ºä¾‹

- **Train GPT-2 with DeepSpeed**
  - https://github.com/microsoft/DeepSpeedExamples/tree/master/training/gpt2

- **Train BERT with Megatron**
  - https://github.com/NVIDIA/Megatron-LM/tree/main/examples

### 11.2 Benchmarké¡¹ç›®

- **MLPerf Training**
  - https://mlcommons.org/en/training-normal-21/
  - ä¸šç•Œæ ‡å‡†æ€§èƒ½æµ‹è¯•

---

## 12. å­¦ä¹ è·¯çº¿å»ºè®®

### é˜¶æ®µ1: åŸºç¡€ (1-2å‘¨)
1. å­¦ä¹ MPIåŸºç¡€
2. ç†è§£é›†åˆé€šä¿¡åŸè¯­
3. æŒæ¡PyTorchåˆ†å¸ƒå¼åŸºç¡€

### é˜¶æ®µ2: æ•°æ®å¹¶è¡Œ (1å‘¨)
1. å®ç°ç®€å•çš„DDPç¨‹åº
2. ç†è§£æ¢¯åº¦åŒæ­¥æœºåˆ¶
3. å­¦ä¹ FSDP/ZeRO

### é˜¶æ®µ3: æ¨¡å‹å¹¶è¡Œ (2-3å‘¨)
1. å®ç°æµæ°´çº¿å¹¶è¡Œ
2. å®ç°å¼ é‡å¹¶è¡Œ
3. ç†è§£é€šä¿¡-è®¡ç®—é‡å 

### é˜¶æ®µ4: é«˜çº§å¹¶è¡Œ (2-3å‘¨)
1. å­¦ä¹ åºåˆ—å¹¶è¡Œ
2. å­¦ä¹ MoEå’Œä¸“å®¶å¹¶è¡Œ
3. å®ç°æ··åˆå¹¶è¡Œç­–ç•¥

### é˜¶æ®µ5: å·¥ç¨‹å®è·µ (æŒç»­)
1. æ€§èƒ½è°ƒä¼˜
2. å¤§è§„æ¨¡é›†ç¾¤éƒ¨ç½²
3. æ•…éšœå®¹é”™å¤„ç†

---

## 13. æ¨èé˜…è¯»é¡ºåº

### å¿…è¯»è®ºæ–‡ (æŒ‰é¡ºåº)
1. Megatron-LM (ç†è§£TP/PPåŸºç¡€)
2. GPipe (ç†è§£æµæ°´çº¿)
3. ZeRO (ç†è§£å†…å­˜ä¼˜åŒ–)
4. Switch Transformers (ç†è§£MoE)
5. Alpa (ç†è§£è‡ªåŠ¨å¹¶è¡Œ)

### å¿…çœ‹æ–‡æ¡£
1. PyTorch DDP Tutorial
2. DeepSpeed Getting Started
3. Megatron-LM Examples
4. NCCL User Guide

### åŠ¨æ‰‹å®è·µé¡¹ç›®
1. å¤ç°æœ¬ä»“åº“æ‰€æœ‰ç¤ºä¾‹
2. è®­ç»ƒä¸€ä¸ªå°å‹GPTæ¨¡å‹
3. å®ç°è‡ªå®šä¹‰å¹¶è¡Œç­–ç•¥
4. æ€§èƒ½å¯¹æ¯”å®éªŒ

---

## 14. ç¤¾åŒºèµ„æº

### è®ºå›ä¸è®¨è®º
- **PyTorch Discuss**
  - https://discuss.pytorch.org/c/distributed/

- **DeepSpeed GitHub Issues**
  - https://github.com/microsoft/DeepSpeed/issues

### å¾®ä¿¡å…¬ä¼—å·
- HPC-AIç§‘æŠ€
- NVIDIAè‹±ä¼Ÿè¾¾
- Microsoft Research
