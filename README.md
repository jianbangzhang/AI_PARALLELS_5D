# MatrixDistributedComputing-5DParallel

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)

> **å¤§è§„æ¨¡çŸ©é˜µåˆ†å¸ƒå¼è®¡ç®—ï¼š5Då¹¶è¡Œå®Œæ•´å®ç°**  
> æ¶µç›–æ•°æ®å¹¶è¡Œ(DP)ã€æµæ°´çº¿å¹¶è¡Œ(PP)ã€å¼ é‡å¹¶è¡Œ(TP)ã€åºåˆ—å¹¶è¡Œ(SP)ã€ä¸“å®¶å¹¶è¡Œ(EP)  
> æä¾›PyTorchã€LibTorch C++ã€çº¯C++ + MPIä¸‰ç§å®ç°

---

## ğŸ“š ç›®å½•

- [é¡¹ç›®ç®€ä»‹](#é¡¹ç›®ç®€ä»‹)
- [5Då¹¶è¡Œæ¦‚è§ˆ](#5då¹¶è¡Œæ¦‚è§ˆ)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [ç›®å½•ç»“æ„](#ç›®å½•ç»“æ„)
- [å®ç°è¯¦æƒ…](#å®ç°è¯¦æƒ…)
- [æ€§èƒ½å¯¹æ¯”](#æ€§èƒ½å¯¹æ¯”)
- [å­¦ä¹ è·¯çº¿](#å­¦ä¹ è·¯çº¿)
- [è´¡çŒ®æŒ‡å—](#è´¡çŒ®æŒ‡å—)
- [å‚è€ƒèµ„æ–™](#å‚è€ƒèµ„æ–™)
- [è‡´è°¢](#è‡´è°¢)

---

## ğŸ¯ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ª**æ•™å­¦å¯¼å‘**çš„åˆ†å¸ƒå¼è®¡ç®—ä»“åº“ï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘è€…ç†è§£å’Œå®è·µå¤§è§„æ¨¡æ·±åº¦å­¦ä¹ ä¸­çš„å¹¶è¡Œç­–ç•¥ã€‚æˆ‘ä»¬æä¾›ï¼š

- âœ… **5ç§å¹¶è¡Œæ–¹å¼**çš„å®Œæ•´å®ç°
- âœ… **3ç§ç¼–ç¨‹æ¡†æ¶**ï¼šPyTorch / LibTorch C++ / çº¯C++ + MPI
- âœ… **æ¸è¿›å¼ç¤ºä¾‹**ï¼šä»åŸºç¡€åˆ°é«˜çº§
- âœ… **è¯¦å°½æ–‡æ¡£**ï¼šç†è®º + ä»£ç  + æ€§èƒ½åˆ†æ
- âœ… **å¯è¿è¡Œä»£ç **ï¼šæ‰€æœ‰ç¤ºä¾‹å‡å¯ç›´æ¥è¿è¡Œ

### é€‚ç”¨äººç¾¤

- ğŸ“ æ·±åº¦å­¦ä¹ ç ”ç©¶è€…å’Œå·¥ç¨‹å¸ˆ
- ğŸ’» é«˜æ€§èƒ½è®¡ç®—å¼€å‘è€…
- ğŸ“– å¯¹å¤§æ¨¡å‹è®­ç»ƒæ„Ÿå…´è¶£çš„å­¦ä¹ è€…
- ğŸ¢ éœ€è¦éƒ¨ç½²åˆ†å¸ƒå¼ç³»ç»Ÿçš„å›¢é˜Ÿ

---

## ğŸŒŸ 5Då¹¶è¡Œæ¦‚è§ˆ

| å¹¶è¡Œæ–¹å¼ | æ ¸å¿ƒæ€æƒ³ | åˆ‡åˆ†å¯¹è±¡ | ä¸»è¦ä¼˜åŠ¿ | é€‚ç”¨åœºæ™¯ |
|---------|---------|---------|---------|---------|
| **æ•°æ®å¹¶è¡Œ (DP)** | å¤åˆ¶æ¨¡å‹ï¼Œåˆ†å‰²æ•°æ® | æ•°æ®æ‰¹æ¬¡ | å®ç°ç®€å•ï¼Œçº¿æ€§åŠ é€Ÿ | å°æ¨¡å‹ï¼Œå¤§æ•°æ®é›† |
| **æµæ°´çº¿å¹¶è¡Œ (PP)** | åˆ†å±‚æ‰§è¡Œï¼Œæµæ°´ä¼ é€’ | æ¨¡å‹å±‚ | å‡å°‘å†…å­˜ï¼Œæ”¯æŒæ·±æ¨¡å‹ | è¶…æ·±ç½‘ç»œ |
| **å¼ é‡å¹¶è¡Œ (TP)** | åˆ‡åˆ†å¼ é‡ï¼Œå¹¶è¡Œè®¡ç®— | çŸ©é˜µ/å¼ é‡ | å†…å­˜é«˜æ•ˆï¼Œæ”¯æŒè¶…å¤§å±‚ | å¤§å‹Transformer |
| **åºåˆ—å¹¶è¡Œ (SP)** | åˆ†å‰²åºåˆ—é•¿åº¦ | åºåˆ—ç»´åº¦ | æ¿€æ´»å†…å­˜ä¼˜åŒ– | è¶…é•¿åºåˆ— |
| **ä¸“å®¶å¹¶è¡Œ (EP)** | åˆ†å¸ƒå¼ä¸“å®¶ç½‘ç»œ | MoEä¸“å®¶ | ç¨€ç–æ¿€æ´»ï¼Œé«˜å®¹é‡ | ä¸‡äº¿å‚æ•°æ¨¡å‹ |

### æ··åˆå¹¶è¡Œç­–ç•¥

```
5Då¹¶è¡Œ = DP Ã— PP Ã— TP Ã— SP Ã— EP

ç¤ºä¾‹é…ç½® (1024 GPUs):
- DP: 8è·¯  (8ä¸ªæ•°æ®å‰¯æœ¬)
- PP: 8è·¯  (8ä¸ªæµæ°´çº¿é˜¶æ®µ)
- TP: 8è·¯  (8è·¯å¼ é‡å¹¶è¡Œ)
- SP: 2è·¯  (2è·¯åºåˆ—å¹¶è¡Œ)
- EP: 1è·¯  (æ‰€æœ‰ä¸“å®¶åœ¨åŒä¸€ç»„)

æ€»æ¨¡å‹å¤§å° â‰ˆ å•GPUæ¨¡å‹å¤§å° Ã— PP Ã— TP Ã— EP
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

#### ç¡¬ä»¶
- å¤šGPUæœåŠ¡å™¨ (æ¨è4+ GPUs)
- NVIDIA GPU (compute capability â‰¥ 7.0)
- InfiniBandæˆ–é«˜é€Ÿç½‘ç»œ (å¤šèŠ‚ç‚¹æ—¶)

#### è½¯ä»¶
- Linuxæ“ä½œç³»ç»Ÿ (Ubuntu 20.04+æ¨è)
- CUDA 11.8+
- Python 3.8+
- GCC 9.0+

### å¿«é€Ÿå®‰è£…

#### 1. PyTorchç¯å¢ƒ

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/your-username/MatrixDistributedComputing-5DParallel.git
cd MatrixDistributedComputing-5DParallel

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n dist-parallel python=3.10
conda activate dist-parallel

# å®‰è£…ä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt
```

#### 2. LibTorch C++ ç¯å¢ƒ

```bash
# ä¸‹è½½LibTorch
wget https://download.pytorch.org/libtorch/cu118/libtorch-cxx11-abi-shared-with-deps-2.1.0%2Bcu118.zip
unzip libtorch-cxx11-abi-shared-with-deps-2.1.0+cu118.zip

# è®¾ç½®ç¯å¢ƒå˜é‡
export LIBTORCH_PATH=/path/to/libtorch
export LD_LIBRARY_PATH=$LIBTORCH_PATH/lib:$LD_LIBRARY_PATH
```

#### 3. MPIç¯å¢ƒ

```bash
# Ubuntu/Debian
sudo apt-get install mpich libmpich-dev

# æˆ–è€…å®‰è£…OpenMPI
sudo apt-get install openmpi-bin openmpi-common libopenmpi-dev

# éªŒè¯å®‰è£…
mpirun --version
```

### è¿è¡Œç¬¬ä¸€ä¸ªç¤ºä¾‹

#### æ•°æ®å¹¶è¡Œ (PyTorch)

```bash
cd 01-data-parallelism/pytorch

# å•èŠ‚ç‚¹å¤šGPU
torchrun --nproc_per_node=4 dp_basic.py

# å¤šèŠ‚ç‚¹ (åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œ)
# Node 0:
torchrun --nnodes=2 --node_rank=0 --master_addr=<MASTER_IP> --master_port=29500 --nproc_per_node=4 dp_basic.py

# Node 1:
torchrun --nnodes=2 --node_rank=1 --master_addr=<MASTER_IP> --master_port=29500 --nproc_per_node=4 dp_basic.py
```

#### æµæ°´çº¿å¹¶è¡Œ (LibTorch)

```bash
cd 02-pipeline-parallelism/libtorch

# ç¼–è¯‘
mkdir build && cd build
cmake -DCMAKE_PREFIX_PATH=$LIBTORCH_PATH ..
make -j8

# è¿è¡Œ (3ä¸ªè¿›ç¨‹å¯¹åº”3ä¸ªæµæ°´çº¿é˜¶æ®µ)
mpirun -np 3 ./pp_basic
```

#### å¼ é‡å¹¶è¡Œ (çº¯C++)

```bash
cd 03-tensor-parallelism/cpp

# ç¼–è¯‘
make

# è¿è¡Œ
mpirun -np 4 ./tp_mpi
```

---

## ğŸ“ ç›®å½•ç»“æ„

```
MatrixDistributedComputing-5DParallel/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                        # æœ¬æ–‡ä»¶
â”œâ”€â”€ ğŸ“„ requirements.txt                 # Pythonä¾èµ–
â”œâ”€â”€ ğŸ“„ LICENSE
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                            # è¯¦ç»†æ–‡æ¡£
â”‚   â”œâ”€â”€ 00-introduction.md              # åˆ†å¸ƒå¼è®¡ç®—æ€»ä½“ä»‹ç»
â”‚   â”œâ”€â”€ 01-data-parallelism.md          # DPç†è®ºä¸å®è·µ
â”‚   â”œâ”€â”€ 02-pipeline-parallelism.md      # PPç†è®ºä¸å®è·µ
â”‚   â”œâ”€â”€ 03-tensor-parallelism.md        # TPç†è®ºä¸å®è·µ
â”‚   â”œâ”€â”€ 04-sequence-parallelism.md      # SPç†è®ºä¸å®è·µ
â”‚   â”œâ”€â”€ 05-expert-parallelism.md        # EPç†è®ºä¸å®è·µ
â”‚   â”œâ”€â”€ 06-hybrid-parallelism.md        # æ··åˆå¹¶è¡Œç­–ç•¥
â”‚   â””â”€â”€ setup-guide.md                  # è¯¦ç»†å®‰è£…æŒ‡å—
â”‚
â”œâ”€â”€ ğŸ“‚ 01-data-parallelism/             # æ•°æ®å¹¶è¡Œ
â”‚   â”œâ”€â”€ pytorch/                        # PyTorchå®ç°
â”‚   â”‚   â”œâ”€â”€ dp_basic.py                 # åŸºç¡€DDP
â”‚   â”‚   â”œâ”€â”€ dp_fsdp.py                  # Fully Sharded DP
â”‚   â”‚   â”œâ”€â”€ dp_matrix_multiply.py       # çŸ©é˜µä¹˜æ³•ç¤ºä¾‹
â”‚   â”‚   â””â”€â”€ run.sh                      # è¿è¡Œè„šæœ¬
â”‚   â”œâ”€â”€ libtorch/                       # LibTorchå®ç°
â”‚   â””â”€â”€ cpp/                            # çº¯C++å®ç°
â”‚
â”œâ”€â”€ ğŸ“‚ 02-pipeline-parallelism/         # æµæ°´çº¿å¹¶è¡Œ
â”‚   â”œâ”€â”€ pytorch/
â”‚   â”‚   â”œâ”€â”€ pp_gpipe.py                 # GPipeå®ç°
â”‚   â”‚   â”œâ”€â”€ pp_manual.py                # æ‰‹åŠ¨æµæ°´çº¿
â”‚   â”‚   â””â”€â”€ pp_1f1b.py                  # 1F1Bè°ƒåº¦
â”‚   â”œâ”€â”€ libtorch/
â”‚   â””â”€â”€ cpp/
â”‚
â”œâ”€â”€ ğŸ“‚ 03-tensor-parallelism/           # å¼ é‡å¹¶è¡Œ
â”‚   â”œâ”€â”€ pytorch/
â”‚   â”‚   â”œâ”€â”€ tp_megatron.py              # Megatroné£æ ¼
â”‚   â”‚   â”œâ”€â”€ tp_column_parallel.py       # åˆ—å¹¶è¡Œ
â”‚   â”‚   â””â”€â”€ tp_row_parallel.py          # è¡Œå¹¶è¡Œ
â”‚   â”œâ”€â”€ libtorch/
â”‚   â””â”€â”€ cpp/
â”‚
â”œâ”€â”€ ğŸ“‚ 04-sequence-parallelism/         # åºåˆ—å¹¶è¡Œ
â”‚   â”œâ”€â”€ pytorch/
â”‚   â”‚   â”œâ”€â”€ sp_basic.py                 # åŸºç¡€SP
â”‚   â”‚   â””â”€â”€ sp_ring_attention.py        # Ring Attention
â”‚   â”œâ”€â”€ libtorch/
â”‚   â””â”€â”€ cpp/
â”‚
â”œâ”€â”€ ğŸ“‚ 05-expert-parallelism/           # ä¸“å®¶å¹¶è¡Œ
â”‚   â”œâ”€â”€ pytorch/
â”‚   â”‚   â”œâ”€â”€ ep_moe.py                   # MoEå®ç°
â”‚   â”‚   â”œâ”€â”€ ep_switch_router.py         # Switchè·¯ç”±
â”‚   â”‚   â””â”€â”€ ep_load_balance.py          # è´Ÿè½½å‡è¡¡
â”‚   â”œâ”€â”€ libtorch/
â”‚   â””â”€â”€ cpp/
â”‚
â”œâ”€â”€ ğŸ“‚ 06-hybrid-parallelism/           # æ··åˆå¹¶è¡Œ
â”‚   â”œâ”€â”€ pytorch/
â”‚   â”‚   â”œâ”€â”€ hybrid_3d.py                # 3Då¹¶è¡Œ (DP+PP+TP)
â”‚   â”‚   â”œâ”€â”€ hybrid_4d.py                # 4Då¹¶è¡Œ
â”‚   â”‚   â””â”€â”€ hybrid_5d.py                # 5Då¹¶è¡Œ
â”‚   â””â”€â”€ examples/
â”‚       â””â”€â”€ train_llama.py              # LLaMAè®­ç»ƒç¤ºä¾‹
â”‚
â”œâ”€â”€ ğŸ“‚ common/                          # å…¬å…±å·¥å…·
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ matrix_generator.py         # çŸ©é˜µç”Ÿæˆ
â”‚   â”‚   â”œâ”€â”€ profiler.py                 # æ€§èƒ½åˆ†æ
â”‚   â”‚   â””â”€â”€ visualizer.py               # å¯è§†åŒ–
â”‚   â””â”€â”€ benchmarks/
â”‚       â””â”€â”€ benchmark_all.py            # æ€§èƒ½æµ‹è¯•è„šæœ¬
â”‚
â”œâ”€â”€ ğŸ“‚ examples/                        # å®Œæ•´åº”ç”¨ç¤ºä¾‹
â”‚   â”œâ”€â”€ gpt2_training/                  # GPT-2è®­ç»ƒ
â”‚   â”œâ”€â”€ bert_pretraining/               # BERTé¢„è®­ç»ƒ
â”‚   â””â”€â”€ llama_inference/                # LLaMAæ¨ç†
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                           # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_dp.py
â”‚   â”œâ”€â”€ test_pp.py
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ ğŸ“‚ scripts/                         # å®ç”¨è„šæœ¬
    â”œâ”€â”€ install_all.sh                  # ä¸€é”®å®‰è£…
    â”œâ”€â”€ benchmark_cluster.sh            # é›†ç¾¤æµ‹è¯•
    â””â”€â”€ visualize_results.py            # ç»“æœå¯è§†åŒ–
```

---

## ğŸ’» å®ç°è¯¦æƒ…

### æ•°æ®å¹¶è¡Œ (DP)

#### æ ¸å¿ƒåŸç†
```python
# ä¼ªä»£ç 
for each_epoch:
    for each_batch:
        # æ¯ä¸ªGPUå¤„ç†ä¸åŒçš„æ•°æ®æ‰¹æ¬¡
        local_loss = model(local_data)
        local_loss.backward()
        
        # æ¢¯åº¦åŒæ­¥ (AllReduce)
        all_reduce(gradients)
        
        # æ‰€æœ‰GPUä½¿ç”¨ç›¸åŒçš„æ¢¯åº¦æ›´æ–°
        optimizer.step()
```

#### å…³é”®ä»£ç ç‰‡æ®µ
```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# åˆå§‹åŒ–è¿›ç¨‹ç»„
dist.init_process_group("nccl")

# åŒ…è£…æ¨¡å‹
model = DDP(model, device_ids=[local_rank])

# DDPè‡ªåŠ¨å¤„ç†æ¢¯åº¦åŒæ­¥
loss.backward()
optimizer.step()
```

### æµæ°´çº¿å¹¶è¡Œ (PP)

#### 1F1Bè°ƒåº¦ç¤ºæ„å›¾
```
æ—¶é—´ â†’
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
GPU 0: F0  F1  F2  F3  B0  B1  B2  B3
GPU 1:  â•°â”€â†’F0  F1  F2  F3  B0  B1  B2
GPU 2:      â•°â”€â†’F0  F1  F2  F3  B0  B1
GPU 3:          â•°â”€â†’F0  F1  F2  F3  B0

F = Forward, B = Backward
```

### å¼ é‡å¹¶è¡Œ (TP)

#### åˆ—å¹¶è¡Œç¤ºæ„å›¾
```
å®Œæ•´çŸ©é˜µ:         åˆ—åˆ‡åˆ†:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”¬â”€â”€â”€â”
â”‚    W    â”‚  â†’   â”‚Wâ‚€ â”‚Wâ‚ â”‚
â”‚ [KÃ—N]   â”‚      â”‚   â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”´â”€â”€â”€â”˜
                 GPU0 GPU1

Y = X @ W  â†’  [Yâ‚€, Yâ‚] = X @ [Wâ‚€, Wâ‚]
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### å®éªŒç¯å¢ƒ
- **ç¡¬ä»¶**: 8Ã— NVIDIA A100 80GB
- **ç½‘ç»œ**: NVLink (600 GB/s)
- **ä»»åŠ¡**: çŸ©é˜µä¹˜æ³• (8192Ã—8192 Ã— 8192Ã—8192)

### æ€§èƒ½æµ‹è¯•ç»“æœ

| å¹¶è¡Œç­–ç•¥ | GPUæ•°é‡ | ååé‡ (TFLOPS) | åŠ é€Ÿæ¯” | å†…å­˜å ç”¨ |
|---------|--------|----------------|--------|---------|
| å•GPU | 1 | 156 | 1.0Ã— | 100% |
| DP (DDP) | 8 | 1,210 | 7.8Ã— | 100% |
| DP (FSDP) | 8 | 1,180 | 7.6Ã— | 12.5% |
| PP (4 stages) | 8 | 980 | 6.3Ã— | 25% |
| TP (8-way) | 8 | 1,150 | 7.4Ã— | 12.5% |
| 3D (DPÃ—PPÃ—TP) | 8 | 1,100 | 7.1Ã— | 12.5% |

### é€šä¿¡å¼€é”€åˆ†æ

```
é€šä¿¡æ—¶é—´å æ¯”:
DP:  ~15% (æ¢¯åº¦AllReduce)
PP:  ~8%  (æ¿€æ´»ä¼ é€’)
TP:  ~20% (å¼ é‡AllReduce/AllGather)
SP:  ~12% (åºåˆ—AllGather)
EP:  ~25% (AllToAllè·¯ç”±)
```

### æ‰©å±•æ€§æµ‹è¯•

```python
# å¼±æ‰©å±• (Weak Scaling)
# å›ºå®šæ¯GPUè´Ÿè½½ï¼Œå¢åŠ GPUæ•°é‡

GPUs:  1    2    4    8    16   32
æ•ˆç‡:  100% 98%  95%  90%  85%  78%

# å¼ºæ‰©å±• (Strong Scaling)
# å›ºå®šæ€»è´Ÿè½½ï¼Œå¢åŠ GPUæ•°é‡

GPUs:  1    2    4    8    16   32
åŠ é€Ÿ:  1.0Ã— 1.9Ã— 3.7Ã— 7.0Ã— 12.8Ã— 22.1Ã—
```

---

## ğŸ“– å­¦ä¹ è·¯çº¿

### åˆçº§ (1-2å‘¨)

**ç›®æ ‡**: ç†è§£åŸºç¡€æ¦‚å¿µï¼Œè¿è¡Œç®€å•ç¤ºä¾‹

1. âœ… **Day 1-2**: é˜…è¯» `docs/00-introduction.md`
2. âœ… **Day 3-5**: å­¦ä¹ æ•°æ®å¹¶è¡Œ (DP)
   - é˜…è¯» `docs/01-data-parallelism.md`
   - è¿è¡Œ `01-data-parallelism/pytorch/dp_basic.py`
   - ä¿®æ”¹batch sizeå’ŒGPUæ•°é‡
3. âœ… **Day 6-7**: ç†è§£é›†åˆé€šä¿¡
   - å­¦ä¹  AllReduceã€AllGather ç­‰åŸè¯­
   - è¿è¡Œ `common/utils/communication_demo.py`

### ä¸­çº§ (2-3å‘¨)

**ç›®æ ‡**: æŒæ¡æ¨¡å‹å¹¶è¡Œï¼Œå®ç°è‡ªå·±çš„å¹¶è¡Œç­–ç•¥

1. âœ… **Week 1**: æµæ°´çº¿å¹¶è¡Œ (PP)
   - å®ç°ç®€å•çš„2é˜¶æ®µæµæ°´çº¿
   - å¯¹æ¯”GPipeå’Œ1F1Bè°ƒåº¦
2. âœ… **Week 2**: å¼ é‡å¹¶è¡Œ (TP)
   - å®ç°åˆ—å¹¶è¡Œå’Œè¡Œå¹¶è¡Œ
   - åˆ†æé€šä¿¡å¼€é”€
3. âœ… **Week 3**: åºåˆ—å¹¶è¡Œ (SP)
   - ç†è§£åºåˆ—åˆ‡åˆ†ç­–ç•¥
   - å®ç°Ring Attention

### é«˜çº§ (3-4å‘¨)

**ç›®æ ‡**: æ··åˆå¹¶è¡Œï¼Œæ€§èƒ½ä¼˜åŒ–ï¼Œå®é™…éƒ¨ç½²

1. âœ… **Week 1-2**: ä¸“å®¶å¹¶è¡Œ (EP)
   - å®ç°ç®€å•MoE
   - è´Ÿè½½å‡è¡¡ä¼˜åŒ–
2. âœ… **Week 3**: æ··åˆå¹¶è¡Œ
   - 3Då¹¶è¡Œå®ç°
   - å‚æ•°æœç´¢å’Œè°ƒä¼˜
3. âœ… **Week 4**: å¤§æ¨¡å‹è®­ç»ƒ
   - è®­ç»ƒGPT-2æˆ–LLaMA
   - æ€§èƒ½profilingå’Œä¼˜åŒ–

---

## ğŸ› ï¸ å¼€å‘ä¸è°ƒè¯•

### å¸¸è§é—®é¢˜

#### 1. NCCLåˆå§‹åŒ–å¤±è´¥
```bash
# æ£€æŸ¥NCCLç‰ˆæœ¬
python -c "import torch; print(torch.cuda.nccl.version())"

# è®¾ç½®è°ƒè¯•ä¿¡æ¯
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=ALL
```

#### 2. OOM (Out of Memory)
```python
# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
model.gradient_checkpointing_enable()

# å‡å°‘batch size
batch_size = batch_size // 2

# ä½¿ç”¨æ··åˆç²¾åº¦
from torch.cuda.amp import autocast
with autocast():
    output = model(input)
```

#### 3. é€šä¿¡å¡æ­»
```bash
# æ£€æŸ¥ç½‘ç»œè¿æ¥
ping <other_node_ip>

# æ£€æŸ¥é˜²ç«å¢™
sudo ufw status

# ä½¿ç”¨Glooåç«¯ (CPU)
dist.init_process_group("gloo")
```

### æ€§èƒ½åˆ†æå·¥å…·

```bash
# PyTorch Profiler
python -m torch.utils.bottleneck your_script.py

# NVIDIA Nsight Systems
nsys profile --trace=cuda,nvtx python your_script.py

# è‡ªå®šä¹‰profiling
python common/utils/profiler.py --script your_script.py
```

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿å„ç§å½¢å¼çš„è´¡çŒ®ï¼

### å¦‚ä½•è´¡çŒ®

1. **Fork** æœ¬ä»“åº“
2. **åˆ›å»º**ä½ çš„ç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. **æäº¤**ä½ çš„æ”¹åŠ¨ (`git commit -m 'Add some AmazingFeature'`)
4. **æ¨é€**åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. **æäº¤** Pull Request

### è´¡çŒ®ç±»å‹

- ğŸ› Bugä¿®å¤
- âœ¨ æ–°åŠŸèƒ½å®ç°
- ğŸ“ æ–‡æ¡£æ”¹è¿›
- ğŸ¨ ä»£ç ä¼˜åŒ–
- ğŸ§ª æµ‹è¯•ç”¨ä¾‹
- ğŸ“Š æ€§èƒ½benchmark

### ä»£ç è§„èŒƒ

```bash
# Pythonä»£ç é£æ ¼
black .
flake8 .
mypy .

# C++ä»£ç é£æ ¼
clang-format -i src/*.cpp
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

### æ ¸å¿ƒè®ºæ–‡

1. **Megatron-LM** - NVIDIA (2019)  
   *Training Multi-Billion Parameter Language Models*  
   [arXiv:1909.08053](https://arxiv.org/abs/1909.08053)

2. **ZeRO** - Microsoft (2020)  
   *Memory Optimizations Toward Training Trillion Parameter Models*  
   [arXiv:1910.02054](https://arxiv.org/abs/1910.02054)

3. **GPipe** - Google (2019)  
   *Easy Scaling with Micro-Batch Pipeline Parallelism*  
   [arXiv:1811.06965](https://arxiv.org/abs/1811.06965)

4. **Switch Transformers** - Google (2021)  
   *Scaling to Trillion Parameter Models with MoE*  
   [arXiv:2101.03961](https://arxiv.org/abs/2101.03961)

### æ¡†æ¶æ–‡æ¡£

- **PyTorch Distributed**: https://pytorch.org/tutorials/beginner/dist_overview.html
- **DeepSpeed**: https://www.deepspeed.ai/
- **Megatron-LM**: https://github.com/NVIDIA/Megatron-LM
- **Colossal-AI**: https://colossalai.org/

### æ¨èä¹¦ç±

- ã€ŠDistributed Systemsã€‹ by Maarten van Steen
- ã€ŠHigh Performance Computingã€‹ by Charles Severance
- ã€ŠProgramming Massively Parallel Processorsã€‹ by David Kirk

---

## ğŸ™ è‡´è°¢

æœ¬é¡¹ç›®å‚è€ƒå’Œå€Ÿé‰´äº†ä»¥ä¸‹ä¼˜ç§€å¼€æºé¡¹ç›®ï¼š

- [DeepSpeed](https://github.com/microsoft/DeepSpeed) - Microsoft
- [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) - NVIDIA
- [Colossal-AI](https://github.com/hpcaitech/ColossalAI) - HPC-AI Tech
- [FairScale](https://github.com/facebookresearch/fairscale) - Meta
- [Alpa](https://github.com/alpa-projects/alpa) - UC Berkeley

ç‰¹åˆ«æ„Ÿè°¢æ‰€æœ‰ä¸ºåˆ†å¸ƒå¼è®­ç»ƒæŠ€æœ¯åšå‡ºè´¡çŒ®çš„ç ”ç©¶è€…å’Œå¼€å‘è€…ï¼

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT License](LICENSE) å¼€æºã€‚

---

## ğŸ“¬ è”ç³»æ–¹å¼

- **Issues**: [GitHub Issues](https://github.com/your-username/MatrixDistributedComputing-5DParallel/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-username/MatrixDistributedComputing-5DParallel/discussions)
- **Email**: your.email@example.com

---

## â­ Star History

å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª â­ï¸ï¼

[![Star History Chart](https://api.star-history.com/svg?repos=your-username/MatrixDistributedComputing-5DParallel&type=Date)](https://star-history.com/#your-username/MatrixDistributedComputing-5DParallel&Date)

---

<div align="center">
  <strong>è®©å¤§è§„æ¨¡åˆ†å¸ƒå¼è®¡ç®—è§¦æ‰‹å¯åŠ</strong>
  <br>
  Made with â¤ï¸ by the community
</div>
